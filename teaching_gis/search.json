[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mapping Economic Landscapes",
    "section": "",
    "text": "About this course",
    "crumbs": [
      "About this course"
    ]
  },
  {
    "objectID": "index.html#suggested-citation-for-materials-of-this-course",
    "href": "index.html#suggested-citation-for-materials-of-this-course",
    "title": "Mapping Economic Landscapes",
    "section": "Suggested citation for materials of this course",
    "text": "Suggested citation for materials of this course\n@misc{minano_mapppingecon_course2024,\n  author = {Alba Miñano-Mañero},\n  title = {Mapping Economic Landscapes: A Course on Geographic Data Science},\n  year = {2024}\n}",
    "crumbs": [
      "About this course"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation guidelines",
    "section": "",
    "text": "Installing QGIS\nQGIS is an open-source, cross-platform, and free geographic information system. For this course, we will use the latest stable version (the long-term release, LTR), QGIS 3.34 Prizren, released in November 2023. QGIS always offers two main versions: the most recent release version and the stable version. On the one hand, the most recent release version has more features and has been tested by developers, but it is still undergoing community testing, implying it may have unresolved bugs. On the other hand, the LTR version has already undergone the community testing process, so it contains fewer bugs, but it also has fewer features.",
    "crumbs": [
      "Installation guidelines"
    ]
  },
  {
    "objectID": "installation.html#installing-qgis",
    "href": "installation.html#installing-qgis",
    "title": "Installation guidelines",
    "section": "",
    "text": "Note\n\n\n\nBear in mind that if a new LTR version of QGIS is available while you are using this course, you can download and use it instead of sticking to QGIS 3.34 Prizren. However, be aware that some functionalities and the execution of certain algorithms may differ from what is presented in this course. In such cases, you are encouraged to use the course notes to consider how these changes might affect your workflow.\n\n\n\nTo install QGIS, visit the official download webpage.\nSelect your operating system and the long-term release. In case the active LTR is different from QGIS 3.34 Prizren, and you wish to stick to that installation, you will have to find it by clicking in the All releases tab.\n\nFollow the on-screen instructions of the installer to complete de installation.\nVerify the installation by opening QGIS and setting the language to English. To do this and many other customizations (e.g., background color, etc.), you can access the options by clicking on the  icon in the Settings tab. This way, you will overwrite your user profile preferences for QGIS, so that every time you open a project with that profile, those preferences will be applied by default.\nTo change the language, click ‘Override System Locale’ and change the language in ‘User Interface Translation’ and accept. When you reopen QGIS, the language will have changed.\n\nFollowing along will be easier if you leverage the processing toolbox. You can activate on the ‘Processing’ tab.\nIf you are unable to install it, you will have to first install the plugin. To do so, access the tab Plugins → Manage and Install Plugins. Look for Processing on the search tab and install the plugin. After, you should be able to activate the toolbox.",
    "crumbs": [
      "Installation guidelines"
    ]
  },
  {
    "objectID": "installation.html#installing-a-python-environment-for-geographic-data-science.",
    "href": "installation.html#installing-a-python-environment-for-geographic-data-science.",
    "title": "Installation guidelines",
    "section": "Installing a Python environment for Geographic Data Science.",
    "text": "Installing a Python environment for Geographic Data Science.\nAlthough desktop GIS software is useful for getting started and learning basic concepts, programming languages give us much more flexibility to continue advancing and making the most of the data.\nFor this course, we will focus on Python for the following reasons:\n\nPython is widely used, which increases the availability of resources that end users can leverage.\nPython is free and open-source.\nPython is connected to many software programs, including QGIS and ArcGIS. This type of connection with other software allows you to extend the functionalities of software packages and customize solutions for your specific data analysis needs.\n\nHowever, setting up a Python environment that allows us to work with geographic data is not straightforward. We need an environment with all the packages/libraries that enable us to work with geographic data (e.g., geopandas). Also, we need to have them installed in a way such that they are compatible with each other. Achieving this is not always easy:\n\n\n\nSource:xkcd\n\n\nTo be able to have mirror environments, I propose doing the following:\n\nInstall Python via Anaconda downloads. Find the corresponding installer for your operating system. Once the installation is complete, the Anaconda navigator should open automatically in your web browse\n\nJust with a native Python installation (either via Anaconda or via the Python official website), we would be equipped to write and run code directly from our terminal. However, this approach is rather unintuitive, especially for beginners who are just getting started with programming. That’s where text editors come into play. Text editors provide a more user-friendly interface for writing and organizing code, offering features like syntax highlighting, auto-completion, and project management tools, making the coding process more intuitive and efficient for programmers of any level.\n\nInstall the reference editor (aka Integrated Development Environment) for the course VSCode.\nVSCode is a very popular editor because it supports almost any language and, by downloading the right extensions, we can adapt the coding to all levels of programming.\nIn this course, I will use all the Python extensions and the Jupyter Notebook ones (python, pylance, jupyter, jupyter cell tags, jupyter keymap, jupyter notebook renderers, jupyter slide show, python indent, python debugger, Python extended)\n\nWith the native Python installation, we get a subset of basic packages, but in this environment, we need to include the packages/libraries that we need for our particular tasks.\nPackages are extensions to the basic Python environment developed by the community. In simple words, we could say they are a collection of scripts that define a new set of functions, data types, and methods that allow further manipulation of our data. Because the architecture of all of these packages is highly interlinked, it is easy to run into incompatibilities when installing our packages.\nThat’s why Python distributors like Anaconda become useful, particularly because it allows us to install conda, which is a very powerful package and environment manager.\nIn particular, when we use conda to install a package it downloads a version of the package that has already been compiled for our specific operating system and architecture. Furthermore, conda resolves package dependencies automatically, avoiding conflicts and installation failures. It also enables users to create isolated environments for projects, ensuring clean and reproducible development environments.\n\nOpen the Anaconda Prompt (Windows) or directly the terminal in Mac and Linux. To verify the installation type conda list to see the list of installed packages and their versions.\n\nTo avoid conflicts during the installation with other installations you might have, create a new conda environment that we will use for the course.\nFrom the same terminal, type: conda create -n course_gds. It will indicate the location and ask you to type ‘y’ and click enter to proceed. This has created a new environment, named ‘course_gds’\n\nTo install the packages in this environment, we will use the environments maintained by Dani Arribas-Bel, Professor of Geographic Data Science at the University of Liverpool. Depending on your computer, you should download the following linked files:\n\nMac with M1 chip\nMac with Intel chip\nWindows\nUbuntu\n\nThese files are like configuration files, containing all the dependencies and settings for a conda environment. This file allows users to recreate the exact environment with all necessary dependencies by running a command on the Conda terminal, making it easier to share and reproduce environments across different systems. This means that if we all install the same configuration file, we will be sharing the same computational environment (i.e., same packages, versions, environment name).\nI recommed that you do not move these files around and leave them on the default Downloads folder.\nActivate the environment you created in step 4 by typing conda activate course_gds. With path_to_txt_file being the location of the file downloaded in step 5, type in the activated environment: conda install --file path_to_txt_file\nTo verify the installation, after the installation is complete, type in the same terminal python.\n\nAlso, open VSCode and in case you haven’t installed the extensions of step 2, install them. After, as final verification try opening a new Jupyter Notebook (File → New File → Jupyter Notebook ) and selecting as kernel the new environment: Select kernel → Python environments → the environment you have created (i.e., course_gds). Now, we should be ready to go.",
    "crumbs": [
      "Installation guidelines"
    ]
  },
  {
    "objectID": "s1_intro_python.html",
    "href": "s1_intro_python.html",
    "title": "1  Course foundations: Jupyter Notebooks, Python syntax, and NumPy",
    "section": "",
    "text": "1.1 What is Python?\nPython is a high-level, general-purpose programming language designed by Guido van Rossum in the late 1980s. It acts as a translator, abstracting away the details of computers and enabling us to communicate with them using a language that’s easier for humans to understand.\nPython emphasizes readability, making it straightforward to write and understand code. It supports various programming paradigms, including procedural, where we provide a sequence of instructions; object-oriented, where we create objects with data and perform actions on them; and functional programming, where code is organized around functions.\nWith a vast ecosystem of libraries and frameworks, Python is widely used across diverse applications, from web development and data analysis to artificial intelligence and scientific computing. Over the years, Python has transitioned from a specialized scientific computing language to a key player in data science, machine learning, and software development, thanks to its extensive open-source libraries and its adaptability to construct sophisticated data applications.",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course foundations:\nJupyter Notebooks, Python syntax, and NumPy</span>"
    ]
  },
  {
    "objectID": "s1_intro_python.html#jupyter-notebooks",
    "href": "s1_intro_python.html#jupyter-notebooks",
    "title": "1  Course foundations: Jupyter Notebooks, Python syntax, and NumPy",
    "section": "1.2 Jupyter Notebooks",
    "text": "1.2 Jupyter Notebooks\nWe will interact with Python code through Jupyter Notebooks on VSCode. Jupyter Notebooks, as well as the popular Jupyter Lab, are all part of the Jupyter Project, which revolves around the provision of tools and standards for interactive computing across different programming languages (Julia, Python, R) through computational notebooks.\nComputational notebook, like Jupyter Notebooks, merge code, plain language explanations, data, and visualizations into a shareable document. Therefore, a notebook provides a fast and interactive platform for writing code, exploring data, creating visualizations, and sharing ideas. In other words, we can consolidate the entire data workflow behind our analysis into a single file (a notebook). This computational approach is based on the belief that hands-on interaction with code is the most effective way to learn and program.\nAt the end of the day, notebooks capture interactive session inputs and outputs along with explanatory text, providing a comprehensive computational record. The notebook file, saved with the .ipynb extension for Jupyter ones, is internally a set of JSON files, allowing for version control and easy sharing. Additionally, notebooks are exportable to various static formats.\nTherefore, notebooks replace traditional console-based interactive computing by introducing a web-based application that captures the entire computation process, from code development and documentation to execution and result communication. The Jupyter Notebook integrates two elements:\n\nA web application (although I recommend VSCode rather than the browser-based extension): This browser-based editing program enables interactive authoring of computational notebooks, offering a swift environment for code prototyping, data exploration, visualization, and idea sharing.\nComputational Notebook documents: These shareable documents combine computer code, plain language explanations, data, visualizations, and interactive controls.\n\nThe Jupyter notebook interacts with kernels, which are implementations of the Jupyter interactive computing protocol specific to different programming languages. In plain words, kernels are the separate process that interprets and executes our code in a given programming language.\nFrom Jupyter on VSCode we can open a new Jupyter Notebook on File → New File → Jupyter Notebook. When we open a new notebook, we will find the following elements:\n\n\n\nNotebooks on VSCode\n\n\n\nToolbar of VSCode.\nName of our new notebook: notice that when a white point appears, it is because there are unsaved changes. Notice that notebooks have the .ipynb extension.\nRun all: will run all cells of the notebook sequentially.\nKernel: allows us to specify the kernel where we want to run our notebook.\nNotebook toolbar: allows to run and edit cells.\nIndicator of type of cell (Python or markdown)\nCell\nAdd new cells\n\nThe notebook is composed of a series of cells, each acting as a multiline text input field. These cells can be executed by pressing Shift-Enter, clicking the “Play” button in the toolbar, or selecting “Cell” and then “Run” from the menu bar. The behavior of a cell during execution depends on its type, with three main types: code cells, markdown cells, and raw cells. By default, every cell starts as a code cell, but its type can be changed using a drop-down menu in the toolbar. This menu becomes active when you click on the cell type indicator in the bottom-left corner of the cell (e.g., where it says “Python” in the image below).\n\n\n\nNotebooks on VSCode\n\n\nMain types of cells:\n\nCode cells: allow you to edit and write new code with syntax highlighting and tab completion. The programming language used depends on the kernel, with the default kernel (IPython) executing Python code.\nWhen a code cell is executed, its computation outputs are displayed in the notebook as the cell’s output. This output can include text, matplotlib figures, HTML tables (such as those used in pandas for data analysis), and more.\nMarkdown cells: let you document the computational process in a literate manner by combining descriptive text with code. In IPython, this is done using the Markdown language, which provides a simple way to emphasize text, create lists, and structure documents with headings.\nHeadings created with hash signs become clickable links in the notebook, offering navigational aids and structural hints when exporting to formats like PDFs. When a Markdown cell is executed, the Markdown code is converted into formatted rich text, and you can also include arbitrary HTML for further customization.\nRaw cells: allow you to write output directly without it being evaluated by the kernel when the notebook is run. They are useful for including preformatted text or code that should not be processed or executed.\n\nWorkflow:\nA workflow in a Jupyter Notebook runs sequentially. As in a script, there is a line hierarchy within cells and a sequential hierarchy between cells. However, notebooks offer the advantage of allowing edits of cells separately multiple times until achieving the desired outcome, rather than rerunning separate scripts.\nWhen working on a computational problem, you can organize related ideas into cells and progress incrementally, moving forward once previous parts function correctly, which is very useful when running codes that have snippets that take long. If, at any point, we want to interrupt running a cell, we can achieve that by pressing the interrupt cell. Also, we can restart the kernel to shut down the computational process.\nWhen we run a cell, the notebook sends that process to the kernel and prints directly the execution output. The execution order appears in square brackets to the left of the printed output. Notice that when we run a notebook, all cells run in the same kernel. This means that whatever we run in a given cell will remain active unless we restart the notebook.\nAlso, whenever we shut down a notebook (either by restarting the kernel or just saving and closing the notebook), all of our processes and outputs that have not been saved to disk will be erased.\n\n\n\n\n\n\nMy tip\n\n\n\nWhile notebooks are highly useful for writing and explaining work processes, particularly to other people, they may not be as practical for large research projects. Instead, I often work concurrently on a script that I run in a notebook, executing it step by step until all lines of code run smoothly. Once I have finished the code, I only save the script and run it in the terminal.",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course foundations:\nJupyter Notebooks, Python syntax, and NumPy</span>"
    ]
  },
  {
    "objectID": "s1_intro_python.html#bascis-of-python-syntax",
    "href": "s1_intro_python.html#bascis-of-python-syntax",
    "title": "1  Course foundations: Jupyter Notebooks, Python syntax, and NumPy",
    "section": "1.3 Bascis of Python syntax",
    "text": "1.3 Bascis of Python syntax\nPython syntax is determined by the fact that Python is:\n\nPython is an interpreted language. By this, we mean that the Python interpreter will run a program by executing and evaluating one line at a time and sequentially. In other words, the source code is executed by the interpreter line-by-line without the need for compilation into machine code beforehand.\nPython is an object-oriented programming language. Everything we define in our code exists within the interpreter as a Python object, meaning it will have its associated attributes and methods.\nAttributes are values associated with an object. The class of an object defines which attributes it will have. Typically, we call objects with the syntax object.attribute_name.\nMethods are functions associated with objects and can access the objects’ attributes. Typically, we use methods with the syntax object.method_name().\n\nFor instance:\n\nx = 5.2\nprint('What part of X is real?', x.real) # This is an attribute of the variable x \nprint('Is x an integer?', x.is_integer()) # this is a method\nprint('What type of variable is x?',type(x)) # this is a function\n\nWhat part of X is real? 5.2\nIs x an integer? False\nWhat type of variable is x? &lt;class 'float'&gt;\n\n\n\n1.3.1 Creating variables\n\n\n\n\n\n\nImportant\n\n\n\nNothing prevents Python from rewritting variables. So if we first do x=5 and then x=6 we will automatically rewrite the variable x to 6.\n\n\nVariables allow assigning names to objects stored in memory for use in our program. This call-by-object reference system means that once we assign a name to an object, we can access the object using that reference name. For example, in the code snippet above, the variable 'x' is referencing the memory object storing 5.2. We can create a new variable x2 that contains the same elements as x:\n\nx2 = x\nx2\n\n5.2\n\n\nBoth variables are the same because they are both referring to the same memory object. However, because numbers are immutable objects, changes to one will not affect the other.\n\nx2 = x2 + 5\nprint(\"This is 'x2' variable after we sum 5 to it:\", x2)\nprint(\"This is our originary 'x', which hasn't changed\", x)\n\nThis is 'x2' variable after we sum 5 to it: 10.2\nThis is our originary 'x', which hasn't changed 5.2\n\n\n\n\n\n\n\n\nOperation signs\n\n\n\nAdditions(+), substractions(-), multiplication (*), division (/), exponent(**), remainder of division aka modulo (%)\n\n\nStrings and tuples (more on this later) are also immutable objects, but there are also mutable objects like lists. In the case of mutable objects, if we create two variables referencing the same, changing one will affect the other because both variables refer to the same memory object that has been modified.\n\ny = ['abc','def']\ny2 = y\ny2.append('ghi')\nprint(\"This is 'y2' variable after we add a new string:\", y2)\nprint(\"This is our orignary 'y', but...\", y)\n\nThis is 'y2' variable after we add a new string: ['abc', 'def', 'ghi']\nThis is our orignary 'y', but... ['abc', 'def', 'ghi']\n\n\n\nprint(y)\n\n['abc', 'def', 'ghi']\n\n\nWe can also assign variable names to different objects in the same code line, separating them through commas.\n\nfirst_letters,second_letters, third_letters = 'abc','def','ghi'\nprint(first_letters)\nprint(second_letters)\nprint(third_letters)\n\nabc\ndef\nghi\n\n\n\n\n1.3.2 Naming variables: good practices\nJust as in writing regular text, coding also involves an idiosyncratic coding style that often results in differences between two individuals’ codes, even if they are accomplishing the same task or using the same functions. This divergence is especially noticeable in variable naming. However, whether we are more of a concise or descriptive type, there are some rules and good practices for everyone:\n\n\n\n\n\n\nWarning\n\n\n\nPython is case sensitive, meaning that variables with different capitalization are considered distinct (i.e., Abc, abc, and ABC are all different)\n\n\n\nNames can only contain upper and lower case letters, underscores (_),and numbers.\nNames can never start with a digit number, only letters or or underscores.\nThey cannot be keywords (Python language reserved words) (help(keywords) will show which words are).\nWe typically use only lowercase letters for variables and reserve uppercase letters for parameters or constants.\nNames should be self-explaining and balance out brevity and explicability. Typically, we reserve names for variables, verbs for attributes, and adjectives for booleans (true/false variables).\nWe can use names in any language, but in general, English is preferred so that anyone can follow the code.\nAvoid using built-in function names because that will overwrite the function (i.e., if we write type we will no longer be able to use type to access the `type’ of variables).\n\n\n\n1.3.3 Data types\nUnlike some other programming languages, Python dynamically infers the type of a variable based on the assigned value. We can access the type of variables using the type() function.\nPrimitive data types are strings, integers, floats, booleans, and None. While I am hard-coding some examples for booleans, it’s more common for them to be generated as the outcome of assessing a condition within a function (i.e., Am I Alba? True). In fact, booleans are typically obtained by comparisons and can be manipulated with logical operations (and, not, or)\n\nthis_is_int = 7\ntype(this_is_int)\n\nint\n\n\n\nthis_is_float = 5.2\ntype(this_is_float)\n\nfloat\n\n\n\nthis_is_a_string= 'abc'\ntype(this_is_a_string)\n\nstr\n\n\n\nthis_is_bool =True\ntype(this_is_bool)\n\nbool\n\n\n\n# More booleans examples\nprint('5 &gt; 3', 5 &gt; 3)      \nprint('5 &gt; 5\\t', 5 &gt; 5)       \nprint('5 != 4.9', 5 != 4.9)\n\n5 &gt; 3 True\n5 &gt; 5    False\n5 != 4.9 True\n\n\n\nthis_is_None = None\ntype(this_is_None)\n\nNoneType\n\n\nBesides the primitive datatypes, there are also containers datatypes. Containers are a collection of objects with a particular given structure. They can be tuples, lists, or dictionaries.\nTuples:\nFixed sequence of immutable Python objects. They are defined as a sequence of elements, separated by a comma between brackets (()), and can mix primitive data types.\nWe can access its elements using indexing, which we call as name_of_tuple[i], where i indicates the position of the element we want. From left to right, we start counting from 0 on the first element. From right to left, we start counting on (-1) as the last.\n\n\n\n\n\n\nNote\n\n\n\nNotice that for Python, the = is not the mathematical equality sign. It simply establishes an assignment saying the left-hand side is the right-hand side. When we want to make comparisons to check equality, we use ==.\n\n\n\nthis_is_tuple = (2,3,4)\nthis_is_tuple_str = ('a','b','c')\nthis_is_tuple_mix = (2,'b',True)\nthis_is_nested = ('a',(2,3,4),'b')\n\nprint(type(this_is_tuple), type(this_is_tuple_str), type(this_is_tuple_mix), type(this_is_nested))\nprint(this_is_tuple[0])\n\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n2\n\n\n\nthis_is_tuple[4]=8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nWhile tuples are immutable (don’t support item assignment), we can operate with them in different ways. We can concatenate them using the + sign, we can repeat them an n number of times using *n*, and we can unpack them.\n\nprint(this_is_tuple + this_is_tuple_str + this_is_tuple_mix) # concatenating\nprint(this_is_tuple_mix*3) # repeating \nunpack1, unpack2, unpack3 = this_is_tuple_str # unpack \nprint(unpack1)\n\n#If we use `*_` in the unpack, we will discard some elements\n\n*_,only_third_e, a = this_is_tuple # unpack \nprint(only_third_e)\n\n(2, 3, 4, 'a', 'b', 'c', 2, 'b', True)\n(2, 'b', True, 2, 'b', True, 2, 'b', True)\na\n3\n\n\n\nthis_is_tuple\n\n(2, 3, 4)\n\n\n\nunpack1, unpack2,*_  = this_is_tuple\nprint(unpack2)\n\n3\n\n\n\nonly_first_e, *_,a = this_is_tuple # unpack \nprint(a)\n\n4\n\n\nLists:\nModifiable sequence of mutable Python objects (i.e., a tuple that we can change). They are also defined as a comma separated sequence of elements, but differently from tuples, with square brackets ([]).\n\nthis_is_list = ['a',4,['a',True]]\nprint(this_is_list)\n\n['a', 4, ['a', True]]\n\n\nWe can slice them in the same way as tuples.\n\nthis_is_list[2]\n\n['a', True]\n\n\n\nthis_is_list\n\n['a', 4, ['a', True]]\n\n\n\nprint(this_is_list[2]) # 3rd element\nprint(this_is_list[0:2]) # starts at 0, ends at 2-1 (i.e., 1 which is the second element)\n\n['a', True]\n['a', 4]\n\n\nWe can extend them, using append() and, also, we can modify them by inserting elements in specfic positions using insert(positions,what we want to insert).\n\nthis_is_list.insert(2,'Hola')\nprint(this_is_list)\n\n['a', 4, 'Hola', ['a', True]]\n\n\n\nthis_is_list.append(['a','b','c'])\nprint(this_is_list)\nthis_is_list.append('hello')\nprint(this_is_list)\n\n# let's inset hola as the second element\nthis_is_list.insert(2,'Hola')\nprint(this_is_list)\n\n['a', 4, 'Hola', ['a', True], ['a', 'b', 'c']]\n['a', 4, 'Hola', ['a', True], ['a', 'b', 'c'], 'hello']\n['a', 4, 'Hola', 'Hola', ['a', True], ['a', 'b', 'c'], 'hello']\n\n\nWe can also remove elements in a given position using pop(index) and the first occurence of a given value with remove(value).\n\nthis_is_list.remove('a')\n\n\nthis_is_list\n\n[4, 'Hola', 'Hola', ['a', True], ['a', 'b', 'c'], 'hello']\n\n\n\nthis_is_list.pop(2)\nprint(this_is_list)\nthis_is_list.append('a')\nthis_is_list.remove('a')\nprint(this_is_list)\n\n[4, 'Hola', ['a', True], ['a', 'b', 'c'], 'hello']\n[4, 'Hola', ['a', True], ['a', 'b', 'c'], 'hello']\n\n\nDictionaries\nUnordered collection of items. Items consist on pairs of Python objects named keys and values. Each key is associated with a unique value, like in a dictionary entry, which allows accessing, retrieving and modifying its values. Dictionaries are typically created using curly braces as {key1:value1,key2:value2}.\n\nweekday_activities = {\n    \"Monday\": [\"Go to work\", \"Attend meetings\", \"Exercise\", \"Cook dinner\"],\n    \"Tuesday\": [\"Work on projects\", \"Run errands\", \"Visit the gym\"],\n    \"Wednesday\": [\"Work remotely\", \"Attend class\", \"Grocery shopping\"],\n    \"Thursday\": [\"Client meetings\", \"Volunteer\", \"Cook dinner\"],\n    \"Friday\": [\"Finish work tasks\", \"Meet friends for dinner\", \"Movie night\"],\n}\n\n\nweekday_activities['Monday']\n\n['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner']\n\n\nTo access one of the elements, we can look for the corresponding key.\nWe can also obtain all keys and values, through methods keys(), values(), items(). The results of these methods, although they migh seems like lists, cannot be subscripted.\n\nprint(weekday_activities.keys())\nprint(weekday_activities.values())\nprint(weekday_activities.items())\n\ndict_keys(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'])\ndict_values([['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'], ['Work on projects', 'Run errands', 'Visit the gym'], ['Work remotely', 'Attend class', 'Grocery shopping'], ['Client meetings', 'Volunteer', 'Cook dinner'], ['Finish work tasks', 'Meet friends for dinner', 'Movie night']])\ndict_items([('Monday', ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner']), ('Tuesday', ['Work on projects', 'Run errands', 'Visit the gym']), ('Wednesday', ['Work remotely', 'Attend class', 'Grocery shopping']), ('Thursday', ['Client meetings', 'Volunteer', 'Cook dinner']), ('Friday', ['Finish work tasks', 'Meet friends for dinner', 'Movie night'])])\n\n\nWe can add new entries by specifying a new key and value pair.\n\nweekday_activities['Saturday']='Fondue night'\nweekday_activities\n\n{'Monday': ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'],\n 'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': ['Finish work tasks', 'Meet friends for dinner', 'Movie night'],\n 'Saturday': 'Fondue night'}\n\n\nIt is also possible to use .update() to add new entries and modify other.\n\nweekday_activities.update({'Friday':'Birthday Party','Sunday':'Hike'})\nweekday_activities\n\n{'Monday': ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'],\n 'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': 'Birthday Party',\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\nNotice, however that if we already have an entry with a given key and we perform a new assign or update, we will overwrite it.\n\nweekday_activities['Friday']='Cinema'\nweekday_activities\n\n{'Monday': ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'],\n 'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': 'Cinema',\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\nTo add extra elements to an existing entry, we can use append. However, to be able to have this funcionality we should always write the values of the entries as list (i.e., in square brackets [])\n\nweekday_activities\n\n{'Monday': ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'],\n 'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': 'Cinema',\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\n\nweekday_activities['Friday']=['Cinema']\nweekday_activities['Friday'].append('Dinner out')\n\n\nweekday_activities\n\n{'Monday': ['Go to work', 'Attend meetings', 'Exercise', 'Cook dinner'],\n 'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': ['Cinema', 'Dinner out'],\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\nWe can again use pop to remove elements specifying a key. Equivalently, we can use del to remove an entire entry.\n\nweekday_activities.pop('Monday')\nweekday_activities\n\n{'Tuesday': ['Work on projects', 'Run errands', 'Visit the gym'],\n 'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': ['Cinema', 'Dinner out'],\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\n\ndel weekday_activities['Tuesday']\nweekday_activities\n\n{'Wednesday': ['Work remotely', 'Attend class', 'Grocery shopping'],\n 'Thursday': ['Client meetings', 'Volunteer', 'Cook dinner'],\n 'Friday': ['Cinema', 'Dinner out'],\n 'Saturday': 'Fondue night',\n 'Sunday': 'Hike'}\n\n\nUsing items() we iterate through each entry in the dictionary. To do this, for each entry, we unpack the two elements that appear in the tuple generated by .items()\n\nweekday_activities.items()\n\ndict_items([('Wednesday', ['Work remotely', 'Attend class', 'Grocery shopping']), ('Thursday', ['Client meetings', 'Volunteer', 'Cook dinner']), ('Friday', ['Cinema', 'Dinner out']), ('Saturday', 'Fondue night'), ('Sunday', 'Hike')])\n\n\n\nfor keys, values in weekday_activities.items():\n    print('My schefule for', keys, 'is', values)\n\nMy schefule for Wednesday is ['Work remotely', 'Attend class', 'Grocery shopping']\nMy schefule for Thursday is ['Client meetings', 'Volunteer', 'Cook dinner']\nMy schefule for Friday is ['Cinema', 'Dinner out']\nMy schefule for Saturday is Fondue night\nMy schefule for Sunday is Hike\n\n\nAlso, if we have a collecion of keys and values, we can write them as a dictionary using .zip(k,v). This will automatically assign the keys to the values in an ordered manner (i.e., first element of the key list cooresponds to first element of the value list).\n\nbday_dict = {} # initialize empty \nkey_list = ('September','November','December','January','May','June')\nvalue_list=(\"Kazuharu's birthday\",\"Joël's birthday\",[\"Utso's birthday\",\"Christian's birthday\"],\"Alba's birthday\",\"Dmitri's birthday\",\"Yang's Birthday\")\nfor key, value in zip(key_list, value_list):\n    bday_dict[key] = value\nbday_dict\n\n{'September': \"Kazuharu's birthday\",\n 'November': \"Joël's birthday\",\n 'December': [\"Utso's birthday\", \"Christian's birthday\"],\n 'January': \"Alba's birthday\",\n 'May': \"Dmitri's birthday\",\n 'June': \"Yang's Birthday\"}\n\n\n\n\n1.3.4 Functions\nSometimes, our code repeats the same few lines multiple times, or we are sharing the same lines in different scripts. Functions in Python allow us to encapsulate a set of instructions and reuse them throughout our code. This helps us avoid repeating the same code multiple times and makes our code more organized and easier to maintain while the complexity of the function is abstracted away.\nIn essence, functions contain three elements: the arguments, the commands we execute on them (i.e., the body of the function), and the return values. They have the following structure:\ndef function_name(arg1, arg2):\n    # do something with argument 1 and argument 2 to get the result r3\n    return r3\nIt is also possible to avoid returning anything by specifying return None or nothing at all or even using print() expressions.\n\ndef function_with_return(x,y):\n    return x+y\n\ndef function_with_print(x,y):\n    print(x+y)\n    print(x+2)\n\n\nfunction_with_return(4,6)\n\n10\n\n\n\nr1 = function_with_return(4,6)\n\n\nr1\n\n10\n\n\n\nfunction_with_print(4,6)\n\n10\n6\n\n\n\nr2 = function_with_print(4,6)\n\n10\n6\n\n\n\nprint(r2)\n\nNone\n\n\n\nx=5\ndef function_with_print(x,y):\n    print(x+y)\n\n    print(x+2)\n    return x+2\n\n\nfunction_with_print(4, 6)\n\n10\n6\n\n\n6\n\n\nAlthough a function can have multiple returns, it cannot have more than one return line (unless they are part of some conditions) because the function stops evaluating the code after it reaches the first return.\n\ndef returns_two_vars(x,y):\n    return x+y, x*2\n\nreturns_two_vars(2,3)\n\n(5, 4)\n\n\nAnd we could access them with our unpacking strategy:\n\nr1,r2=returns_two_vars(2,3)\nprint('r1:',  r1)\nprint('r2:', r2)\n\nr1: 5\nr2: 4\n\n\n\ndef returns_two_vars(x,y):\n    return x+y\n    return x*2\n\n\nreturns_two_vars(2,3)\n\n5\n\n\nNotice if we call as an argument a variable that has been defined outside the function, it will automatically take that value (i.e., it’s global to the function). If the variable is also defined inside, it will be local within the function.\n\nglobal_var = 10\n\ndef example_function(global_var):\n    # Access and modify the global variable\n    print(\"Input you gave me\", global_var) \n    global_var = global_var +5  # Modify the global variable\n    print(\"Inside the function - modified global_var:\", global_var)  \n    \n# Call the function\nexample_function(global_var)\nprint(\"Outside the function - global_var:\", global_var) \n\n# Call the function\nexample_function(7)\nprint(\"Outside the function - global_var:\", global_var) \n\nInput you gave me 10\nInside the function - modified global_var: 15\nOutside the function - global_var: 10\nInput you gave me 7\nInside the function - modified global_var: 12\nOutside the function - global_var: 10\n\n\n\n\n1.3.5 Control flow: branching and looping.\nEvery script and computer program generally consists of instructions that are executed sequentially from top to bottom. This sequence is referred to as the program’s flow. It is possible to alter this sequential flow to include branching or repeating certain instructions. The statements that enable these modifications are collectively known as flow control.\n\n1.3.5.1 Branching: if, elif, else\nBranching refers to the capacity to make decisions and execute distinct sets of statements depending on whether one or more conditions are met. In Python, branching departs from the if statement.\nif condition:\n    do line1\n    do line2\nWhenever the condition is found to be true, the code will perform line1 and line2. The way Python understands line1 and line2 are associated with the if statement is because of identation (i.e., white space to the left of the beginning of a line). The 4 spaces from the left to where do line1 is written constitute the Python ident. We can use the tab key to get the ident and shift+tab to remove one ident. Idents are cumulative, so if we had two blocks, the lines within the second would have to have two idents.\nFor instance, the line below would perform line1 only if the condition is true, and after it would perform line2 for all.\nif condition:\n    do line1\n    \ndo line2\n\nmonth = 'January'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block')\n    print('The entire month of', month, ' is considered to be in the winter')\n\nHey, you are in the idented block\nThe entire month of January  is considered to be in the winter\n\n\n\nmonth = 'June'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block')\n    print('The entire month of', month, ' is considered to be in the winter')\n\nBut what if we still want our branch to do something when the first condition is not met? We can achieve this by specifying a new action with else:\nif condition1:\n    do line1\nelse:\n    do line2\nNotice that, whenever condition1 is not true, the else condition will imply we perform line2.\n\nmonth = 'May'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block')\n    print('The entire month of', month, 'is considered to be in the winter')\nelse: \n    print('The entire month of', month, 'is NOT considered to be in the winter')\n\nThe entire month of May is NOT considered to be in the winter\n\n\nWe can also be more specific and chain conditions using elif. This will start executing the first condition, if it is not true, it will try the second, and so on and so forth. Once it has found a condition for which it is True it will execute the lines under it and will stop evaluating the rest of the conditions. This means that at most one - the first encountered- True statement will be evaluated.\n\nmonth = 'May'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block 1')\n    print('The entire month of', month, 'is considered to be in the winter')\nelif (month=='April') or (month=='May'): \n    print('Hey, you are in idented block 2')\n    print('The entire month of', month, 'is considered to be in the spring')\nelif (month=='July') or (month=='August'): \n    print('Hey, you are in idented block 3')\n    print('The entire month of', month, 'is considered to be in the summer')\nelif (month=='October') or (month=='November'): \n    print('Hey, you are in idented block 4')\n    print('The entire month of', month, 'is considered to be in the autumn')\nelse:\n    print('Hey, you reached the else')\n    print('The month', month, 'is in between two different seasons')    \n\nHey, you are in idented block 2\nThe entire month of May is considered to be in the spring\n\n\n\nmonth = 'January'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block 1')\n    print('The entire month of', month, 'is considered to be in the winter')\n    if month=='January':\n        print('We are in the nested if')\n        print(\"It's cold\")\nelif (month=='April') or (month=='May'): \n    print('Hey, you are in idented block 2')\n    print('The entire month of', month, 'is considered to be in the spring')\nelif (month=='July') or (month=='August'): \n    print('Hey, you are in idented block 3')\n    print('The entire month of', month, 'is considered to be in the summer')\nelif (month=='October') or (month=='November'): \n    print('Hey, you are in idented block 4')\n    print('The entire month of', month, 'is considered to be in the autumn')\nelse:\n    print('Hey, you reached the else')\n    print('The month', month, 'is in between two different seasons')    \n\nHey, you are in the idented block 1\nThe entire month of January is considered to be in the winter\nWe are in the nested if\nIt's cold\n\n\nIf we want a statement to do nothing in Python, we use the pass keyword. This is because Python does not allow an empty block within a conditional statement. It’s akin to being at a crossroads where we have to choose between turning left or right, but instead, we decide to wait and take no action.\n\nmonth = 'April'\nif (month=='January') or (month=='February'):\n    print('Hey, you are in the idented block 1')\n    print('The entire month of', month, 'is considered to be in the winter')\nelif (month=='April') or (month=='May'): \n    pass\nelif (month=='July') or (month=='August'): \n    print('Hey, you are in idented block 3')\n    print('The entire month of', month, 'is considered to be in the summer')\nelif (month=='October') or (month=='November'): \n    print('Hey, you are in idented block 4')\n    print('The entire month of', month, 'is considered to be in the autumn')\nelse:\n    print('Hey, you reached the else')\n    print('The month', month, 'is in between two different seasons')    \n\n\n\n1.3.5.2 Looping: while, for\nSometimes, we also want to repeat the same statement multiple times. This is what we mean when we refer to iteration or looping, which we achieve in python through while and for.\nWhile loops execute a statement whenever the condition is true. We typically use this kind of loop to perform some operation or update in the variable.\nwhile condition:\n    do this\n\ngood_morning='Y'\nwhile good_morning == 'Y':\n    print('Good morning, Joël!')\n    print('Good morning, Alba!')\n    good_morning = 'N'\nprint('Awesome! Enjoy your class')\n\nGood morning, Joël!\nGood morning, Alba!\nAwesome! Enjoy your class\n\n\nNotice that the previous loop would go on and if we hadn’t written the good_morning ='N'. That’s why typically in while loops we update the variable to make sure we stop when we reach a given amount of iterations. In the Notebook version of this section, you’ll be requested to input ‘Y/N’ in the loop so that you can see that unless you press ‘N’ it will continue runing.\n\ngood_morning='Y'\niteration=0\nwhile good_morning == 'Y':\n    print('Good morning, Joël!')\n    print('Good morning, Alba!')\n    iteration += 1\n    if iteration==3:\n        break\n    good_morning = 'Y'\nprint('Awesome! Enjoy your class')\n\nGood morning, Joël!\nGood morning, Alba!\nGood morning, Joël!\nGood morning, Alba!\nGood morning, Joël!\nGood morning, Alba!\nAwesome! Enjoy your class\n\n\nFor loops are used to iterate over elements of a sequence. For instance, in the first example, I will loop over the elements in ‘Cemfi’ and return each of them as an upper case letter.\n\nfor i in \"Cemfi\":\n    print(i.upper())\n\nC\nE\nM\nF\nI\n\n\n\nmonths_of_year = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\n# Loop through the months and add some summer vibes\nfor month in months_of_year:\n    if month == \"June\":\n        print(f\"Get ready to enjoy the summer break, it's {month}!\")\n    elif month == \"July\":\n        print(month,\"is perfect to find reasons to escape from Madrid\") \n    elif month=='August': # the \\n allows to break the print in two lines\n        print(month, 'is when you realize that living in Madrid is like living\\n6 months in winterfell and 6 months in winterHELL\\n') \n        print('The joke in spanish makes more sense and is unrelated to GoT: 6 meses de invierno y 6 meses en el infierno')        \n    else:\n        print(f\"Winter is coming\")\n\nWinter is coming\nWinter is coming\nWinter is coming\nWinter is coming\nWinter is coming\nGet ready to enjoy the summer break, it's June!\nJuly is perfect to find reasons to escape from Madrid\nAugust is when you realize that living in Madrid is like living\n6 months in winterfell and 6 months in winterHELL\n\nThe joke in spanish makes more sense and is unrelated to GoT: 6 meses de invierno y 6 meses en el infierno\nWinter is coming\nWinter is coming\nWinter is coming\nWinter is coming\n\n\nThe function range generates a sequence of number to loop over. It follows the syntax range(start, stop, step):\n\nStart is optional and it starts from 0 unless otherwise specified.\nStop: is always needed. The last number will always be stop-1\nStep: optional, defaults to 1.\n\n\nfor i in range(10):\n    print(i+2)\n\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course foundations:\nJupyter Notebooks, Python syntax, and NumPy</span>"
    ]
  },
  {
    "objectID": "s1_intro_python.html#section",
    "href": "s1_intro_python.html#section",
    "title": "1  Course foundations: Jupyter Notebooks, Python syntax, and NumPy",
    "section": "1.4 ",
    "text": "1.4 \nA module in Python is a program that can be imported into interactive mode or other programs for use. A Python package typically comprises multiple modules. Physically, a package is a directory containing modules and possibly subdirectories, each potentially containing further modules. Conceptually, a package links all modules together using the package name for reference.\nNumPy (Numerical Python) is one of the most common packages used in Python. In fact, numerous computational packages that offer scientific capabilities utilize NumPy’s array objects as a standard interface for data exchange. That’s why, although NumPy doesn’t inherently have scientific capabilities, understanding NumPy arrays and array-based computing principles can save you time in the future.\nNumPy offers many efficient methods for creating and manipulating numerical data arrays. Unlike Python lists, which can accommodate various data types within a single list, NumPy arrays require homogeneity among their elements for efficient mathematical operations. Utilizing NumPy arrays provides advantages such as faster execution and reduced memory consumption compared to Python lists. With NumPy, data storage is optimized through the specification of data types, enhancing code optimization.\n\nimport numpy as np \n\nThe array serves as a fundamental data structure within the NumPy. They represent a grid of values containing information on raw data, element location, and interpretation. Elements share a common data type, known as the array dtype.\nOne method of initializing NumPy arrays involves using Python lists, with nested lists employed for two- or higher-dimensional data structures.\n\na = np.array([1, 2, 3, 4, 5, 6])\n\nWe can access the elements through indexing.\n\na[0]\n\n1\n\n\nArrays are N-Dimensional (that’s why sometimes we refer to them as NDarray). That means that NumPy arrays will encompass vector (1-Dimensions), matrices (2D) or tensors (3D and higher). We can get all the information of the array by checking its attributes.\n\na = np.array([[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]])\n\n\na\n\narray([[1, 2, 3, 4, 5, 6],\n       [1, 2, 3, 4, 5, 6]])\n\n\n\nprint('Dimensions/axes:', a.ndim)\nprint('Shape (size of array in each dimension):', a.shape)\nprint('Size (total number of elements):', a.size)\nprint('Data type:', a.dtype)\n\nDimensions/axes: 2\nShape (size of array in each dimension): (2, 6)\nSize (total number of elements): 12\nData type: int64\n\n\nWe can initialize arrays using different commands depending on what we are aiming at.\nFor instance, the most straightforward case would be to pass a list to np.array() to create one:\n\narr1 = np.array([5,6,7])\narr1\n\narray([5, 6, 7])\n\n\nHowever, sometimes we are more ambiguous and have no information on what our array contains. We just need to be able to initialize an array so that later on, our code, can update it. For this, we typically create arrays of the desired dimensions and fill them with zeros (np.zeros()), ones (np.ones()), with a given value (np.full()) or without initializing (np.empty()).\n\n\n\n\n\n\nTip\n\n\n\nWhen working with large data, np.empty() can be faster and more efficient. Also, large arrays can take up most of your memory and, in those cases, carefully establishing the dtype() can help to manage memory more efficiently (i.e., chose 8 bits over 64 bits.)\n\n\n\nfrom numpy import zeros as nanazeros\n\n\nnanazeros(4)\n\narray([0., 0., 0., 0.])\n\n\n\nnp.ones((2,2))\n\narray([[1., 1.],\n       [1., 1.]])\n\n\nHere you have an example of a 3D array of ones: if has 3 rows, 2 columns and 1 of height (depth)\n\nnp.ones((3,2,1))\n\narray([[[1.],\n        [1.]],\n\n       [[1.],\n        [1.]],\n\n       [[1.],\n        [1.]]])\n\n\nWe can use np.full() to create an array of constant vales that we specify in the fill_value option.\n\nnp.full((2,2,) , fill_value= 4)\n\narray([[4, 4],\n       [4, 4]])\n\n\nWhen we use np.empty() we are creating an unitialized array, in the sense that it is reserving the requested space in memory and returns an array with ‘garbage’ values.\n\nnp.empty(2)\n\narray([-1.53282543e-270,  2.96171364e-036])\n\n\nWith np.linspace() we create an array with values that are equally spaced between the start and endpoint. For instance, in the below code we are creating an array with 5 equally spaced values from 0 to 20.\n\nnp.linspace(0,20,num=5)\n\narray([ 0.,  5., 10., 15., 20.])\n\n\n\n1.4.0.1 Managing array elements.\nArrays accept common operations like sorting, concatenating and finding unique elements.\nFor instance, using the sort() method we can sort elements within an array.\n\narr1 = np.array((10,2,5,3,50,0))\nnp.sort(arr1)\n\narray([ 0,  2,  3,  5, 10, 50])\n\n\nIn multidimensional arrays, we can sort the elements of a given dimension by specifying the axis (0 within columns, 1 across rows)\n\nmat1 = np.array([[1,2,3],[8,1,5]])\nmat1\n\narray([[1, 2, 3],\n       [8, 1, 5]])\n\n\n\nmat1.sort(axis=0)\nmat1\n\narray([[1, 1, 3],\n       [8, 2, 5]])\n\n\nUsing concatenate we can join the elements of two arrays along an existing axis.\n\narr1 = np.array((1,2,3))\narr2 = np.array((6,7,8))\nnp.concatenate((arr1,arr2))\n\narray([1, 2, 3, 6, 7, 8])\n\n\nInstead, if we want to concatenate along a new axis, we use vstack() and hstack()\n\nnp.vstack((arr1,arr2))\n\narray([[1, 2, 3],\n       [6, 7, 8]])\n\n\n\nnp.hstack((arr1,arr2)) # equivalent in this case to concatenate \n\narray([1, 2, 3, 6, 7, 8])\n\n\nIt is also possible to reshape arrays. For instance, let’s reshape the concatenation of arr1 and arr2 to 3 rows and 2 columns\n\narr_c= np.concatenate((arr1,arr2))\narr_c.reshape((3,2))\n\narray([[1, 2],\n       [3, 6],\n       [7, 8]])\n\n\nWe can also perform aggregation functions over all elements, like finding the minimum, maximum, means, sum of elements and much more.\n\nprint(arr1.min())\nprint(arr1.sum())\nprint(arr1.max())\nprint(arr1.mean())\n\n1\n6\n3\n2.0\n\n\nIt is also possible to get only the unique elements of an array or to count how many elements are repeated.\n\narr1 =np.array((1,2,3,3,1,1,5,6,7,8,11,11))\nprint(np.unique(arr1))\nunq, count = np.unique(arr1, return_counts=True)\n\n[ 1  2  3  5  6  7  8 11]\n\n\n\ncount # First element appears 3 times, second 1... \n\narray([3, 1, 2, 1, 1, 1, 1, 2])\n\n\nComparing NumPy arrays is can be done using operators as ==, !=, and the like. Comparisons will result in an array of booleans indicating if the condition is met for the cells.\n\narr1 = np.array(((1,2,3),(4,5,6)))\narr2 = np.array(((1,5,3),(7,2,6)))\narr1==arr2\n\narray([[ True, False,  True],\n       [False, False,  True]])\n\n\n\n\n1.4.0.2 Arithmetic operations with arrays.\nFrom algebraic rules, we can only perform operations on an array and a scalar or with two arrays of the same shape. NumPy arrays support common operations as addition, substraction and multiplication as long as those two conditions are met.\nFor NumPy, operations that happen within cells are what we know as broadcasting. Broadcasting is how NumPy operates between two arrays with different numbers of dimensions but compatible shape.\n\narr1 = np.array(((1,2,3),\n                (4,5,6)))\narr2 = np.array(((10,20,30),\n                 (40,50,60)))\n\nElement-wise addition, substraction and multiplication can be performed with +, - and *.\n\narr1+arr2\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\narr1-arr2\n\narray([[ -9, -18, -27],\n       [-36, -45, -54]])\n\n\n\narr1*arr2\n\narray([[ 10,  40,  90],\n       [160, 250, 360]])\n\n\nTo multiply (*) or divide (/) all elements by an scalar, we just specify the scalar.\n\narr1*10\n\narray([[10, 20, 30],\n       [40, 50, 60]])\n\n\n\narr2/10\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\nMatrix multiplication is achieved with matmul(). Because both of our matrices are 3-by-2, I will transpose one of them so that we can perform matrix multiplicaiton.\n\nnp.matmul(arr1,arr2.T) \n\narray([[140, 320],\n       [320, 770]])",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course foundations:\nJupyter Notebooks, Python syntax, and NumPy</span>"
    ]
  },
  {
    "objectID": "s1_intro_python.html#practice-exercises",
    "href": "s1_intro_python.html#practice-exercises",
    "title": "1  Course foundations: Jupyter Notebooks, Python syntax, and NumPy",
    "section": "1.5 Practice exercises",
    "text": "1.5 Practice exercises\n\nCreate a 1D array with all integer elemenets from 1 to 10 (both included). No hard-coding allowed!\n\n\nShow solution\narray_ex = np.array(range(1,11))\n\n\nFrom array you created in 1, create one that contains all odd elements and one with all even elements.\n\n\nShow solution\nodds = [o for o in array_ex if o%2==1]\neven = [o for o in array_ex if o%2==0]\n\n\nCreate a new array that replaces all elements in 1 that are odd by -1.\n\n\nShow solution\narr_r = array_ex\narr_r = np.where(arr_r%2,arr_r,-1)\n\n\nCreate a 3-by-3 matrix filled with ‘True’ values (i.e., booleans).\n\n\nShow solution\nmat3 = np.full((3,3),fill_value =True,dtype=bool)\n#Congrats! you just created your first mask\n\n\nSuppose you have array a=np.array(['a','b','c','d','e','f','g']) and b = np.array(['g','h','c','a','e','w','g']). Find all elements that are equal. Can you get the position where the elements of both arrays match?\n\n\nShow solution\na=np.array(['a','b','c','d','e','f','g'])\nb = np.array(['g','h','c','a','e','w','g'])\ninter=np.intersect1d(a,b)\n\ninter, apos, bpos = np.intersect1d(a,b,return_indices=True)\n\n\nWrite a function that takes a element an array and prints elements that are divisible by a given number. Try it creating an array from 1 to 20 and printing divisibles by 3.\n\n\nShow solution\ndef print_divisible(array,div):\n    for i in array: \n        if i%div == 0:\n            print(i,'is divisible by', div)\n    return None\n\narr =np.array(range(1,21))\n# call the function\n#print_divisible(arr,3)\n\n\nConsider two matrices, A and B, both of size 100x100, filled with random integer values between 1 and 10. Implement a function to perform element-wise multiplication of these matrices using nested loops. Implement the same operation using Numpy’s vectorized multiplication. Repeart again with matrices of size 1000x1000, 10000x10000, 1000000x1000000 and compare the execution time. Which one is faster?\n\n\nShow solution\nmat_a =np.random.randint(1, 11, size=(1000, 1000))\nmat_b =np.random.randint(1, 11, size=(1000, 1000))\n\ndef multiply_in_loop(m1,m2):\n    if m1.shape != m2.shape:\n        raise ValueError(\"Matrices A and B must have the same shape.\")\n\n    m, n = m1.shape\n    result = np.zeros((m, n))\n\n    for i in range(m):\n        for j in range(n):\n            result[i, j] = m1[i, j] * m2[i, j]\n\n    return result\n\nout= multiply_in_loop(mat_a,mat_b)\n\nout_built = np.multiply(mat_a,mat_b)\n# to check each element is exactly the same\n(out_built == out).sum()",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course foundations:\nJupyter Notebooks, Python syntax, and NumPy</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section2.html",
    "href": "notebooks_md/section2.html",
    "title": "2  Course foundations: data management",
    "section": "",
    "text": "2.1 Data management with Pandas\nPandas is a Python library designed for efficient and practical data analysis tasks (clean, manipulate, analyze). Differently from NumPy, which is designed to work with numerical arrays, Pandas is designed to handle relational or labeled data (i.e., data that has been given a context or meaning through labels).\nPandas and NumPy share some similarities as Pandas builds on NumPy. However, rather than arrays, the main object for 2D data management in Pandas is the DataFrame (equivalent to data.frame in R). DataFrames are essentially matrices with labeled columns and rows that can accommodate mixed datatypes and missing values. DataFrames are composed of rows and columns. Row labels are known as indices (start from 0) and column labels as columns names.\nPandas is a very good tool for:",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course foundations:\ndata management</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section2.html#data-management-with-pandas",
    "href": "notebooks_md/section2.html#data-management-with-pandas",
    "title": "2  Course foundations: data management",
    "section": "",
    "text": "Dealing with missing data\nAdding or deleting columns\nAlign data on labels or not on labels (i.e., merging and joining)\nPerform grouped operations on data sets.\nTransforming other Python data structures to DataFrames.\nMulti-indexing hierarchically.\nLoad data from multiple file types.\nHandling time-series data, as it has specific time-series functionalities.\n\n\n2.1.1 Data structures\nPandas builds on two main data structures: Series and DataFrames. A Series represents a 1D array, while a DataFrame is a 2D labeled array. The easiest way to understand these structures is to think of DataFrames as containers for lower-dimensional data. Specifically, the columns of a DataFrame are composed of Series, and each element within a Series (i.e., the rows of the DataFrame) is an individual scalar value, such as a number or a string. In simple terms, Series are columns made of scalar elements, and DataFrames are collections of Series with assigned labels. The image below represents a DataFrame.\n\n\n\nSource: Pandas documentation\n\n\nAll Pandas data structures are value-mutable (i.e., we can change the values of elements and replace DataFrames) but some are not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame.\n\n\n2.1.2 Creating a DataFrame.\nThere are two primary methods to create DataFrames and Series in Pandas. Firstly, they can be constructed by transforming built-in Python data structures like lists or dictionaries. Secondly, they can be generated by importing data, for example, by reading .csv files. Let’s begin by examining how to create them from Python structures.\nFor any code we want to run that uses pandas, just as we did with NumPy, we have to first import it. In the second line of code of the cell below, I am modifying the display options so that we can see all columns in a dataframe; otherwise, there is a default limit, and some columns get collapsed in previews.\n\nimport pandas\npandas.set_option('display.max_columns', None)\n\nThe first example illustrates how to create a DataFrame from a dictionary.\n\nphds = {'Name': ['Joël Marbet', 'Alba Miñano-Mañero','Dmitri Kirpichev',\n                'Yang Xun','Utso Pal Mustafi',\n                'Christian Maruthiah','Kazuharu Yanagimoto'], \n        'Undergrad University': ['Univeristy of Bern', 'University of València',\n                                'Universidad Complutense de Madrid',\n                                'Renmin University of China',\n                                'Presidency University','University of Melbourne','University of Tokyo'],\n        'Fields':['Monetary Economics','Urban Economics','Applied Economics',\n                'Development Economics','Macro-Finance',\n                'Development Economics','Macroeconomics'],\n        'PhD Desk': [20,10,11,15,18,23,5]}\nphd_df = pandas.DataFrame(phds)\nphd_df\n\n\n\n\n\n\n\n\n\nName\nUndergrad University\nFields\nPhD Desk\n\n\n\n\n0\nJoël Marbet\nUniveristy of Bern\nMonetary Economics\n20\n\n\n1\nAlba Miñano-Mañero\nUniversity of València\nUrban Economics\n10\n\n\n2\nDmitri Kirpichev\nUniversidad Complutense de Madrid\nApplied Economics\n11\n\n\n3\nYang Xun\nRenmin University of China\nDevelopment Economics\n15\n\n\n4\nUtso Pal Mustafi\nPresidency University\nMacro-Finance\n18\n\n\n5\nChristian Maruthiah\nUniversity of Melbourne\nDevelopment Economics\n23\n\n\n6\nKazuharu Yanagimoto\nUniversity of Tokyo\nMacroeconomics\n5\n\n\n\n\n\n\n\n\nTo find out how many rows and columns a dataframe has, we can call the .shape attribute. For instance, the dataframe we just created has 2 rows and 4 columns.\n\nphd_df.shape\n\n(7, 4)\n\n\nWe can work with single columns or subset our dataframe to a set of columns by calling the columns names in square braces []. The returning column will be of the pandas.Series type.\n\nphd_df['Name']\n\n0            Joël Marbet\n1     Alba Miñano-Mañero\n2       Dmitri Kirpichev\n3               Yang Xun\n4       Utso Pal Mustafi\n5    Christian Maruthiah\n6    Kazuharu Yanagimoto\nName: Name, dtype: object\n\n\n\ntype(phd_df['Name'])\n\npandas.core.series.Series\n\n\n\n\n\n\n\n\nMy tip\n\n\n\nUsing double braces ([[]]) to subset a DataFrame results in a DataFrame rather than a Series. This is so because the inner pair of brackets creates a list of columns, while the outer pair allows for data selection from the DataFrame.\n\n\n\nphd_df[['Name']]\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nJoël Marbet\n\n\n1\nAlba Miñano-Mañero\n\n\n2\nDmitri Kirpichev\n\n\n3\nYang Xun\n\n\n4\nUtso Pal Mustafi\n\n\n5\nChristian Maruthiah\n\n\n6\nKazuharu Yanagimoto\n\n\n\n\n\n\n\n\n\ntype(phd_df[['Name']])\n\npandas.core.frame.DataFrame\n\n\nColumns names can be accessed by calling the attribute .columns of a dataframe.\n\nphd_df.columns\n\nIndex(['Name', 'Undergrad University', 'Fields', 'PhD Desk'], dtype='object')\n\n\nWith this in mind, we can copy a selection of our DataFrame to a new one. We use the .copy() method to generate a copy of the data and avoid broadcasting any change to the original frame we are subsetting.\n\nphd_df2 = phd_df[['Name','Fields']].copy()\nphd_df2\n\n\n\n\n\n\n\n\n\nName\nFields\n\n\n\n\n0\nJoël Marbet\nMonetary Economics\n\n\n1\nAlba Miñano-Mañero\nUrban Economics\n\n\n2\nDmitri Kirpichev\nApplied Economics\n\n\n3\nYang Xun\nDevelopment Economics\n\n\n4\nUtso Pal Mustafi\nMacro-Finance\n\n\n5\nChristian Maruthiah\nDevelopment Economics\n\n\n6\nKazuharu Yanagimoto\nMacroeconomics\n\n\n\n\n\n\n\n\nWe could also create a dataframe from other Python data-structures. In the example below, the same DataFrame is created from a list. If we do not specify the column names, it will just give numeric labels to columns from 0 to \\(n\\).\n\nphds_list = [\n    ['Joël Marbet', 'University of Bern', 'Monetary Economics', 20],\n    ['Alba Miñano-Mañero', 'University of València', 'Urban Economics', 10],\n    ['Dmitri Kirpichev', 'Universidad Complutense de Madrid', \n        'Applied Economics', 11],\n    ['Yang Xun', 'Renmin University of China', 'Development Economics', 15],\n    ['Utso Pal Mustafi', 'Presidency University', 'Macro-Finance', 18],\n    ['Christian Maruthiah', 'University of Melbourne', \n        'Development Economics', 23],\n    ['Kazuharu Yanagimoto', 'University of Tokyo', 'Macroeconomics', 5]\n]\n# Create a list of column names\ncolumn_names_list = ['Name', 'Undergraduate University', 'Fields','PhD desk']\n\n# Create a DataFrame from the array, using the list as column names and the tuple as row labels\ndf = pandas.DataFrame(phds_list, columns=column_names_list)\ndf\n\n\n\n\n\n\n\n\n\nName\nUndergraduate University\nFields\nPhD desk\n\n\n\n\n0\nJoël Marbet\nUniversity of Bern\nMonetary Economics\n20\n\n\n1\nAlba Miñano-Mañero\nUniversity of València\nUrban Economics\n10\n\n\n2\nDmitri Kirpichev\nUniversidad Complutense de Madrid\nApplied Economics\n11\n\n\n3\nYang Xun\nRenmin University of China\nDevelopment Economics\n15\n\n\n4\nUtso Pal Mustafi\nPresidency University\nMacro-Finance\n18\n\n\n5\nChristian Maruthiah\nUniversity of Melbourne\nDevelopment Economics\n23\n\n\n6\nKazuharu Yanagimoto\nUniversity of Tokyo\nMacroeconomics\n5\n\n\n\n\n\n\n\n\nA DataFrame can also be created departing from a tuple:\n\nphds_tu = (\n    ('Joël Marbet', 'University of Bern', 'Monetary Economics', 20),\n    ('Alba Miñano-Mañero', 'University of València', 'Urban Economics', 10),\n    ('Dmitri Kirpichev', 'Universidad Complutense de Madrid', \n        'Applied Economics', 11),\n    ('Yang Xun', 'Renmin University of China', 'Development Economics', 15),\n    ('Utso Pal Mustafi', 'Presidency University', 'Macro-Finance', 18),\n    ('Christian Maruthiah', 'University of Melbourne', \n        'Development Economics', 23),\n    ('Kazuharu Yanagimoto', 'University of Tokyo', 'Macroeconomics', 5)\n)\n# Create a list of column names\ncolumn_names_list = ['Name', 'Undergraduate University', 'Fields','PhD desk']\n\n# Create a DataFrame from the array, using the list as column names and the tuple as row labels\ndf = pandas.DataFrame(phds_tu, columns=column_names_list)\ndf\n\n\n\n\n\n\n\n\n\nName\nUndergraduate University\nFields\nPhD desk\n\n\n\n\n0\nJoël Marbet\nUniversity of Bern\nMonetary Economics\n20\n\n\n1\nAlba Miñano-Mañero\nUniversity of València\nUrban Economics\n10\n\n\n2\nDmitri Kirpichev\nUniversidad Complutense de Madrid\nApplied Economics\n11\n\n\n3\nYang Xun\nRenmin University of China\nDevelopment Economics\n15\n\n\n4\nUtso Pal Mustafi\nPresidency University\nMacro-Finance\n18\n\n\n5\nChristian Maruthiah\nUniversity of Melbourne\nDevelopment Economics\n23\n\n\n6\nKazuharu Yanagimoto\nUniversity of Tokyo\nMacroeconomics\n5\n\n\n\n\n\n\n\n\n\ntype(phds_tu)\n\ntuple\n\n\nDataFrames can also receive as data source NumPy arrays:\n\nimport numpy as np \ndata_arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Create a DataFrame from the NumPy array\ndf_arr = pandas.DataFrame(data_arr, columns=['A', 'B', 'C'])\n\ndf_arr\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9\n\n\n\n\n\n\n\n\nSimilarly, we can also transform built-in structures into pandas.Series\n\nweek_series= pandas.Series(['Monday','Tuesday','Wednesday']) # from a list\nweek_series\n\n0       Monday\n1      Tuesday\n2    Wednesday\ndtype: object\n\n\nThe previous line would be equivalent to first creating a list and then passing it as an argument to pandas.Series. We can also specify the row index, which can be very useful for join and concatenate operations. Additionally, we can specify a name\n\nweek_list = ['Monday','Tuesday','Wednesday']\nweek_l_to_series = pandas.Series(week_list,index=range(3,6),name='Week')\nweek_l_to_series\n\n3       Monday\n4      Tuesday\n5    Wednesday\nName: Week, dtype: object\n\n\nWe could convert that series to a pandas.DataFrame() that would have a column name ‘Week’ and as row index (3,4,5)\n\npandas.DataFrame(week_l_to_series)\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\n\n3\nMonday\n\n\n4\nTuesday\n\n\n5\nWednesday\n\n\n\n\n\n\n\n\nSimilarly, Series can also be created from dictionaries as shown below:\n\nweekdays_s = {\n    'Monday': 'Work',\n    'Tuesday': 'Work',\n    'Wednesday': 'Work',\n    'Thursday': 'Work',\n    'Friday': 'Work',\n    'Saturday': 'Weekend',\n    'Sunday': 'Weekend'\n}\ndic_to_series = pandas.Series(weekdays_s,name='Day_type')\n\n\ndic_to_series\n\nMonday          Work\nTuesday         Work\nWednesday       Work\nThursday        Work\nFriday          Work\nSaturday     Weekend\nSunday       Weekend\nName: Day_type, dtype: object\n\n\nMost Pandas operations and methods return DataFrames or Series. For instance, the describe() method, which returns basic statistics of numerical data, returns either a DataFrame or a Series.\n\ndf_arr['A'].describe()\n\ncount    3.0\nmean     4.0\nstd      3.0\nmin      1.0\n25%      2.5\n50%      4.0\n75%      5.5\nmax      7.0\nName: A, dtype: float64\n\n\n\nx = df_arr['A'].describe() # I use describe on a series,  returns a series\ntype(x)\n\npandas.core.series.Series\n\n\n\nx = df_arr[['A']].describe()  # I use describe on a dataframe, returns a dataframe\ntype(x)\n\npandas.core.frame.DataFrame\n\n\n\nx\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\ncount\n3.0\n\n\nmean\n4.0\n\n\nstd\n3.0\n\n\nmin\n1.0\n\n\n25%\n2.5\n\n\n50%\n4.0\n\n\n75%\n5.5\n\n\nmax\n7.0\n\n\n\n\n\n\n\n\n\n\n2.1.3 Importing and exporting data\nBesides creating our own tabular data, Pandas can be used to import and read data into Pandas own structures. Reading different file types (excel, parquets, json…) is supported by tha family of read_* functions. To illustrate it, let’s take a look to the credit car fraud data from Kaggle.\nFirst, we define the location of our data file. If you have a local copy of the course folder, you can specify the path as ../data/card_transdata.csv. This path tells the program to start from the current working directory (i.e., the folder where this notebook is saved), move up one level, and then navigate to the data folder, where the .csv file is stored.\n\nfile_csv ='../data/card_transdata.csv'\ndata = pandas.read_csv(file_csv)\n\nPrinting the first or last n elements of the data is possible using the methods .head(n) and .tail(n), for the first and bottom elements respectively.\n\ndata.head(5)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\ndata.tail(5)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n999995\n2.207101\n0.112651\n1.626798\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n999996\n19.872726\n2.683904\n2.778303\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n999997\n2.914857\n1.472687\n0.218075\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n999998\n4.258729\n0.242023\n0.475822\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n999999\n58.108125\n0.318110\n0.386920\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nIt is also possible to get all the particular data types specifications of our columns:\n\ndata.dtypes # it's an attribute of the dataframe\n\ndistance_from_home                float64\ndistance_from_last_transaction    float64\nratio_to_median_purchase_price    float64\nrepeat_retailer                   float64\nused_chip                         float64\nused_pin_number                   float64\nonline_order                      float64\nfraud                             float64\ndtype: object\n\n\nOr to get a quick technical summary of our imported data, we can use the .info() method\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 8 columns):\n #   Column                          Non-Null Count    Dtype  \n---  ------                          --------------    -----  \n 0   distance_from_home              1000000 non-null  float64\n 1   distance_from_last_transaction  1000000 non-null  float64\n 2   ratio_to_median_purchase_price  1000000 non-null  float64\n 3   repeat_retailer                 1000000 non-null  float64\n 4   used_chip                       1000000 non-null  float64\n 5   used_pin_number                 1000000 non-null  float64\n 6   online_order                    1000000 non-null  float64\n 7   fraud                           1000000 non-null  float64\ndtypes: float64(8)\nmemory usage: 61.0 MB\n\n\nThe summary is telling us that:\n\nWe have a pandas.DataFrame.\nIt has 1,000,000 rows and their index are from 0 to 999,9999\nNone of the collumns has missing data\nAll columns data type is a 64 bit float.\nIt takes up 61 megabytes of our RAM memory.\n\nWhile the read_* families allow to import different sources of data, the to_* allow to export it. In the below example, I am saving a csv that contains only the first 10 rows of the credit transaction data.\n\ndata.head(10).to_csv('./subset.csv')\n\nIf we export other data to the same file, it will typically overwrite the existing one (unless we are specifying sheets in an Excel file, for instance). Also, this happens without warning, so it is important to be consistent in our naming to avoid unwanted overwrites.\n\ndata.head(5).to_csv('./subset.csv')\n\n\n\n2.1.4 Filtering and subsetting dataframes.\nIt is possible to use an approach similar to the selection of columns to be able to subset our dataframe to a given set of rows.\nFor example, let’s extract from the credit transaction data those that happened through online orders.\n\nonline = data[data['online_order']==1]\nonline.head()\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n6\n3.724019\n0.956838\n0.278465\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n9\n8.839047\n2.970512\n2.361683\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nWe can check that in our new subset the variable online_order is always 1.\n\nonline['online_order'].describe()\n\ncount    650552.0\nmean          1.0\nstd           0.0\nmin           1.0\n25%           1.0\n50%           1.0\n75%           1.0\nmax           1.0\nName: online_order, dtype: float64\n\n\n\nonline['online_order'].unique()\n\narray([1.])\n\n\nThat is, to select rows based on a column condition, we first write the condition within []. This selection bracket will give use a series of boolean (i.e., a mask) with True values for those rows that satify the condition.\n\nmask_data = data['online_order']==1\nmask_data\n\n0         False\n1         False\n2          True\n3          True\n4          True\n          ...  \n999995    False\n999996    False\n999997     True\n999998     True\n999999     True\nName: online_order, Length: 1000000, dtype: bool\n\n\n\ntype(mask_data)\n\npandas.core.series.Series\n\n\nThen, when we embrace that condition with the outer [] what we are doing is subsetting the original dataframe to those rows that are True, that is, that satisfy our condition.\nIf our condition involves two discrete values, we can write both conditions within the inner bracket using the | separator for or conditions and & for and. That is, we could write data[(data['var']==x) | (data['var']==y)]. This line of code is equivalent to using the .isin() conditional function with argument [x,y].\nSometimes, it is also useful to get rid of missing values on given columns using the .notna() method.\nIf we want to select rows and columns simultaneously, we can use the .loc[condition,variable to keep]and .iloc[condition,variable to keep] operators. For instance, to extract the distance from home only for the fraudulent transactions we could do:\n\ndata.loc[data['fraud']==1,'distance_from_home'] # first element is the condition,  \n                                            # second one is the column we want. \n\n13          2.131956\n24          3.803057\n29         15.694986\n35         26.711462\n36         10.664474\n             ...    \n999908     45.296658\n999916    167.139756\n999919    124.640118\n999939     51.412900\n999949     15.724799\nName: distance_from_home, Length: 87403, dtype: float64\n\n\n.loc is also useful to replace the value of columns where a row satisfies certain condition. For instance, let’s create a new variable that takes value 1 only for those transactions happenning more than 5km away from home.\n\ndata['more_5km'] = 0\n\ndata.loc[data['distance_from_home'] &gt; 5, 'more_5km'] = 1\n\ndata[data['more_5km']==1]['distance_from_home'].describe()\n\ncount    688790.000000\nmean         37.534356\nstd          76.321463\nmin           5.000021\n25%           9.407164\n50%          17.527437\n75%          37.590001\nmax       10632.723672\nName: distance_from_home, dtype: float64\n\n\nTo extract using indexing of rows and columns we typically rely in iloc instead. In the snippet of code below, I am extracting the rows from 10 to 20 and the second and third column:\n\ndata.iloc[10:21,2:4]\n\n\n\n\n\n\n\n\n\nratio_to_median_purchase_price\nrepeat_retailer\n\n\n\n\n10\n1.136102\n1.0\n\n\n11\n1.370330\n1.0\n\n\n12\n0.551245\n1.0\n\n\n13\n6.358667\n1.0\n\n\n14\n2.798901\n1.0\n\n\n15\n0.535640\n1.0\n\n\n16\n0.516990\n1.0\n\n\n17\n1.846451\n1.0\n\n\n18\n2.530758\n1.0\n\n\n19\n0.307217\n1.0\n\n\n20\n1.838016\n1.0\n\n\n\n\n\n\n\n\nIn whatever selection you are doing, keep in mind that the reasoning is always you first generate a mask and then you get the rows for which the mask holds.\n\n\n2.1.5 Creating new columns\nPandas operates with the traditional mathematical operators in an element-wise fashion, without the need of writing for loops. For instance, to convert the distance from kilometers to miles (i.e., we divide by 1.609)\n\ndata['distance_from_home_km'] = data['distance_from_home']/1.609\ndata\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\nmore_5km\ndistance_from_home_km\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n35.971322\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n1\n6.730853\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n1\n3.164126\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.396870\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n27.464845\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n999995\n2.207101\n0.112651\n1.626798\n1.0\n1.0\n0.0\n0.0\n0.0\n0\n1.371722\n\n\n999996\n19.872726\n2.683904\n2.778303\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n12.350979\n\n\n999997\n2.914857\n1.472687\n0.218075\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.811595\n\n\n999998\n4.258729\n0.242023\n0.475822\n1.0\n0.0\n0.0\n1.0\n0.0\n0\n2.646818\n\n\n999999\n58.108125\n0.318110\n0.386920\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n36.114434\n\n\n\n\n1000000 rows × 10 columns\n\n\n\n\nWe can also operate with more than one columns. Let’s sum the indicators for repeat retailer and used chip so that we can get a new column that directly tell us if a row satisfies both conditions.\n\ndata['sum_2'] = data['repeat_retailer'] + data['used_chip']\ndata\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\nmore_5km\ndistance_from_home_km\nsum_2\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n35.971322\n2.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n1\n6.730853\n1.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n1\n3.164126\n1.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.396870\n2.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n27.464845\n2.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n999995\n2.207101\n0.112651\n1.626798\n1.0\n1.0\n0.0\n0.0\n0.0\n0\n1.371722\n2.0\n\n\n999996\n19.872726\n2.683904\n2.778303\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n12.350979\n2.0\n\n\n999997\n2.914857\n1.472687\n0.218075\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.811595\n2.0\n\n\n999998\n4.258729\n0.242023\n0.475822\n1.0\n0.0\n0.0\n1.0\n0.0\n0\n2.646818\n1.0\n\n\n999999\n58.108125\n0.318110\n0.386920\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n36.114434\n2.0\n\n\n\n\n1000000 rows × 11 columns\n\n\n\n\nBecause sum_2 is not a very intuitive name, let’s take advantage of that to show we can rename columns:\n\ndata = data.rename(columns={'sum_2':'both_conditions'})\ndata\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\nmore_5km\ndistance_from_home_km\nboth_conditions\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n35.971322\n2.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n1\n6.730853\n1.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n1\n3.164126\n1.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.396870\n2.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n27.464845\n2.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n999995\n2.207101\n0.112651\n1.626798\n1.0\n1.0\n0.0\n0.0\n0.0\n0\n1.371722\n2.0\n\n\n999996\n19.872726\n2.683904\n2.778303\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n12.350979\n2.0\n\n\n999997\n2.914857\n1.472687\n0.218075\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.811595\n2.0\n\n\n999998\n4.258729\n0.242023\n0.475822\n1.0\n0.0\n0.0\n1.0\n0.0\n0\n2.646818\n1.0\n\n\n999999\n58.108125\n0.318110\n0.386920\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n36.114434\n2.0\n\n\n\n\n1000000 rows × 11 columns\n\n\n\n\n\n\n2.1.6 Merging tables.\nSometimes, we find that in the process of cleaning data, we need to put together different DataFrames, either vertically or horizontally. This can be achieved concatenating along indexes or, when the dataframes have a common identifier, using .merge()\nLet’s start to show we can concatenate along axis. The default axis is 0, which means that it will concatenate rows (i.e., append dataframes vertically)\n\ndata_1 = data.iloc[0:100]\ndata_2 = data.iloc[100:200]\n\n\nconcat_rows = pandas.concat([data_1,data_2], axis = 0)\nconcat_rows \n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\nmore_5km\ndistance_from_home_km\nboth_conditions\n\n\n\n\n0\n57.877857\n0.311140\n1.945940\n1.0\n1.0\n0.0\n0.0\n0.0\n1\n35.971322\n2.0\n\n\n1\n10.829943\n0.175592\n1.294219\n1.0\n0.0\n0.0\n0.0\n0.0\n1\n6.730853\n1.0\n\n\n2\n5.091079\n0.805153\n0.427715\n1.0\n0.0\n0.0\n1.0\n0.0\n1\n3.164126\n1.0\n\n\n3\n2.247564\n5.600044\n0.362663\n1.0\n1.0\n0.0\n1.0\n0.0\n0\n1.396870\n2.0\n\n\n4\n44.190936\n0.566486\n2.222767\n1.0\n1.0\n0.0\n1.0\n0.0\n1\n27.464845\n2.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n195\n45.031736\n0.055077\n1.312067\n1.0\n0.0\n0.0\n1.0\n0.0\n1\n27.987406\n1.0\n\n\n196\n242.913187\n4.424776\n0.741428\n1.0\n0.0\n0.0\n0.0\n0.0\n1\n150.971527\n1.0\n\n\n197\n4.586564\n3.365070\n2.454288\n1.0\n0.0\n0.0\n0.0\n0.0\n0\n2.850568\n1.0\n\n\n198\n19.153042\n0.403533\n1.111563\n1.0\n0.0\n1.0\n1.0\n0.0\n1\n11.903693\n1.0\n\n\n199\n0.718005\n10.054520\n1.341927\n0.0\n1.0\n0.0\n0.0\n0.0\n0\n0.446243\n1.0\n\n\n\n\n200 rows × 11 columns\n\n\n\n\nIf instead we try to concatenate horizontally, it will by default try to concatenate with the index (unless we specify ignore_index=True). In the below example, since the columns are subset in different set of rows, it just fills with NaN the rows that do not share an index in the other dataframe. We can also use custom columns to align the dataframes specifying the column name in the ‘keys’ options\n\ndata_c1 =data_1[['distance_from_home','repeat_retailer' ]]\ndata_c2 =data_2[['distance_from_last_transaction','used_chip' ]]\ndata_ch = pandas.concat([data_c1,data_c2],axis = 1) \ndata_ch \n\n\n\n\n\n\n\n\n\ndistance_from_home\nrepeat_retailer\ndistance_from_last_transaction\nused_chip\n\n\n\n\n0\n57.877857\n1.0\nNaN\nNaN\n\n\n1\n10.829943\n1.0\nNaN\nNaN\n\n\n2\n5.091079\n1.0\nNaN\nNaN\n\n\n3\n2.247564\n1.0\nNaN\nNaN\n\n\n4\n44.190936\n1.0\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n195\nNaN\nNaN\n0.055077\n0.0\n\n\n196\nNaN\nNaN\n4.424776\n0.0\n\n\n197\nNaN\nNaN\n3.365070\n0.0\n\n\n198\nNaN\nNaN\n0.403533\n0.0\n\n\n199\nNaN\nNaN\n10.054520\n1.0\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nUsing concat for horizontal concatenation can be very useful when we have more than one dataset to concatenate and want to avoid repeated lines using .merge, which by default can only concatenate two dataframes.\n\ndata_c1 =data_1[['distance_from_home','repeat_retailer' ]]\ndata_c2 =data_1[['distance_from_last_transaction','used_chip' ]]\ndata_c3 =data_1[['used_pin_number','online_order']]\ndata_ch = pandas.concat([data_c1,data_c2,data_c3],axis = 1) \ndata_ch \n\n\n\n\n\n\n\n\n\ndistance_from_home\nrepeat_retailer\ndistance_from_last_transaction\nused_chip\nused_pin_number\nonline_order\n\n\n\n\n0\n57.877857\n1.0\n0.311140\n1.0\n0.0\n0.0\n\n\n1\n10.829943\n1.0\n0.175592\n0.0\n0.0\n0.0\n\n\n2\n5.091079\n1.0\n0.805153\n0.0\n0.0\n1.0\n\n\n3\n2.247564\n1.0\n5.600044\n1.0\n0.0\n1.0\n\n\n4\n44.190936\n1.0\n0.566486\n1.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n11.411763\n1.0\n2.233248\n1.0\n0.0\n1.0\n\n\n96\n2.987854\n1.0\n9.315192\n0.0\n0.0\n1.0\n\n\n97\n0.530985\n0.0\n1.501575\n1.0\n0.0\n0.0\n\n\n98\n6.136181\n1.0\n2.579574\n1.0\n1.0\n1.0\n\n\n99\n1.532551\n0.0\n3.097159\n1.0\n0.0\n0.0\n\n\n\n\n100 rows × 6 columns\n\n\n\n\nIf instead we wanted to use the .merge() method, we would have to repeat it twice:\n\ndata_m1 = data_c1.reset_index().merge(\n                                data_c2.reset_index(),\n                                on='index',\n                                how='inner',\n                                validate='1:1')\ndata_m1\n\n\n\n\n\n\n\n\n\nindex\ndistance_from_home\nrepeat_retailer\ndistance_from_last_transaction\nused_chip\n\n\n\n\n0\n0\n57.877857\n1.0\n0.311140\n1.0\n\n\n1\n1\n10.829943\n1.0\n0.175592\n0.0\n\n\n2\n2\n5.091079\n1.0\n0.805153\n0.0\n\n\n3\n3\n2.247564\n1.0\n5.600044\n1.0\n\n\n4\n4\n44.190936\n1.0\n0.566486\n1.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n95\n11.411763\n1.0\n2.233248\n1.0\n\n\n96\n96\n2.987854\n1.0\n9.315192\n0.0\n\n\n97\n97\n0.530985\n0.0\n1.501575\n1.0\n\n\n98\n98\n6.136181\n1.0\n2.579574\n1.0\n\n\n99\n99\n1.532551\n0.0\n3.097159\n1.0\n\n\n\n\n100 rows × 5 columns\n\n\n\n\n\ndata_m1 = data_m1.merge(data_c3.reset_index(),\n                        on='index',\n                        how='inner',\n                        validate='1:1')\ndata_m1\n\n\n\n\n\n\n\n\n\nindex\ndistance_from_home\nrepeat_retailer\ndistance_from_last_transaction\nused_chip\nused_pin_number\nonline_order\n\n\n\n\n0\n0\n57.877857\n1.0\n0.311140\n1.0\n0.0\n0.0\n\n\n1\n1\n10.829943\n1.0\n0.175592\n0.0\n0.0\n0.0\n\n\n2\n2\n5.091079\n1.0\n0.805153\n0.0\n0.0\n1.0\n\n\n3\n3\n2.247564\n1.0\n5.600044\n1.0\n0.0\n1.0\n\n\n4\n4\n44.190936\n1.0\n0.566486\n1.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n95\n11.411763\n1.0\n2.233248\n1.0\n0.0\n1.0\n\n\n96\n96\n2.987854\n1.0\n9.315192\n0.0\n0.0\n1.0\n\n\n97\n97\n0.530985\n0.0\n1.501575\n1.0\n0.0\n0.0\n\n\n98\n98\n6.136181\n1.0\n2.579574\n1.0\n1.0\n1.0\n\n\n99\n99\n1.532551\n0.0\n3.097159\n1.0\n0.0\n0.0\n\n\n\n\n100 rows × 7 columns\n\n\n\n\n\n\n2.1.7 Summary statistics and groupby operations.\nPandas also offers various statistical measures that can be applied to numerical data columns. By default, operations exclude missing data and extend across rows.\n\nprint('Mean distance from home:', data['distance_from_home'].mean())\nprint('Minimum distance from home:',data['distance_from_home'].min())\nprint('Median distance from home:', data['distance_from_home'].median())\nprint('Maximum distance from home:', data['distance_from_home'].max())\n\nMean distance from home: 26.62879219257128\nMinimum distance from home: 0.0048743850667442\nMedian distance from home: 9.967760078697681\nMaximum distance from home: 10632.723672241103\n\n\nIf we want more flexibility we can also specify a series of statistics and the columns we want them to be computed on using .agg\n\ndata.agg(\n    {\n           \"distance_from_home\": [\"min\", \"max\", \"median\", \"skew\"],\n            \"distance_from_last_transaction\": [ \"max\", \"median\", \"mean\"],  \n       }\n)\n\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\n\n\n\n\nmin\n0.004874\nNaN\n\n\nmax\n10632.723672\n11851.104565\n\n\nmedian\n9.967760\n0.998650\n\n\nskew\n20.239733\nNaN\n\n\nmean\nNaN\n5.036519\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nComputing the mean of a dummy variable will tell us the percentage of observations within that category.\n\n\n\ndata['fraud'].mean()  ## 8% are frauds. \n\n0.087403\n\n\nBesides these aggreggating operations, we can also compute them within category or by groups.\n\ndata.groupby(\"fraud\")[\"distance_from_home\"].mean()\n\nfraud\n0.0    22.832976\n1.0    66.261876\nName: distance_from_home, dtype: float64\n\n\nBecause the default return is a pandas.Series, it is useful to convert it to a pandas.DataFrame which can be done as follows:\n\npandas.DataFrame(data.groupby(\"fraud\")[\"distance_from_home\"].mean())\n\n\n\n\n\n\n\n\n\ndistance_from_home\n\n\nfraud\n\n\n\n\n\n0.0\n22.832976\n\n\n1.0\n66.261876\n\n\n\n\n\n\n\n\nNotice that the new indexing variable is automatically set to our groupby one. We can undo this change to recover the groupby variable as accessible:\n\npandas.DataFrame(data.groupby(\"fraud\")[\"distance_from_home\"].mean()).reset_index()\n\n\n\n\n\n\n\n\n\nfraud\ndistance_from_home\n\n\n\n\n0\n0.0\n22.832976\n\n\n1\n1.0\n66.261876",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course foundations:\ndata management</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section2.html#visualization-matplotlib-and-seaborn.",
    "href": "notebooks_md/section2.html#visualization-matplotlib-and-seaborn.",
    "title": "2  Course foundations: data management",
    "section": "2.2 Visualization: Matplotlib and Seaborn.",
    "text": "2.2 Visualization: Matplotlib and Seaborn.\nThe library that will allow us to do data visualization, whether static, animated or interactive, in Python is Matplotlib.\nMatplotlib plots our data on Figures, each one containing axes where data points are defined as x-y coordinates. This means that matplotlib will allow us to manipulate the displays by changing the elements of these two classes:\nFigure:\nThe entire figure that contains the axes, which are the actual plots.\nAxes:\nContains a region for plotting data, and include the x and y axis (and z if is 3D) where we actually plot the data. The figure below, from the package original documentation, illustrates all the elements within a figure.\n\n\n\nSource : Matplotlib documentation\n\n\n\nimport matplotlib.pyplot as plt\n\nFor the following examples, we will use data different from the crad transaction. However, just as in most typical real life situations, we will have to do some pre-processing before we can start plotting our data. In this case, we are going to be using data from the World Health Organization on mortality induced by pollution and country GDP per capita data from the World Bank\n\ndeath_rate = pandas.read_csv('../data/data.csv')\ngdp = pandas.read_csv('../data/gdp-per-capita-worldbank.csv')\n\n\ndeath_rate[(death_rate['Location']=='Algeria') \n            & (death_rate['Dim1']=='Both sexes') \n            & (death_rate['Dim2']=='Total') \n            & (death_rate['IndicatorCode']=='SDGAIRBOD')]\n\n\n\n\n\n\n\n\n\nIndicatorCode\nIndicator\nValueType\nParentLocationCode\nParentLocation\nLocation type\nSpatialDimValueCode\nLocation\nPeriod type\nPeriod\nIsLatestYear\nDim1 type\nDim1\nDim1ValueCode\nDim2 type\nDim2\nDim2ValueCode\nDim3 type\nDim3\nDim3ValueCode\nDataSourceDimValueCode\nDataSource\nFactValueNumericPrefix\nFactValueNumeric\nFactValueUoM\nFactValueNumericLowPrefix\nFactValueNumericLow\nFactValueNumericHighPrefix\nFactValueNumericHigh\nValue\nFactValueTranslationID\nFactComments\nLanguage\nDateModified\n\n\n\n\n2342\nSDGAIRBOD\nAmbient and household air pollution attributab...\nnumeric\nAFR\nAfrica\nCountry\nDZA\nAlgeria\nYear\n2019\nTrue\nSex\nBoth sexes\nSEX_BTSX\nCause\nTotal\nENVCAUSE_ENVCAUSE000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n41.43\nNaN\nNaN\n28.46\nNaN\n55.68\n41 [28-56]\nNaN\nNaN\nEN\n2022-08-25T22:00:00.000Z\n\n\n\n\n\n\n\n\nWe keep the variables we are interested in.\n\nkeep_vars = ['Indicator','Location','ParentLocationCode',\n            'ParentLocation','SpatialDimValueCode',\n            'Location','FactValueNumeric']\n\ndeath_rate = death_rate[(death_rate['Dim1']=='Both sexes') \n                        & (death_rate['Dim2']=='Total') \n                        & (death_rate['IndicatorCode']=='SDGAIRBOD')][keep_vars]\n\n\ndeath_rate\n\n\n\n\n\n\n\n\n\nIndicator\nLocation\nParentLocationCode\nParentLocation\nSpatialDimValueCode\nLocation\nFactValueNumeric\n\n\n\n\n389\nAmbient and household air pollution attributab...\nBahamas\nAMR\nAmericas\nBHS\nBahamas\n10.04\n\n\n450\nAmbient and household air pollution attributab...\nDemocratic Republic of the Congo\nAFR\nAfrica\nCOD\nDemocratic Republic of the Congo\n100.30\n\n\n453\nAmbient and household air pollution attributab...\nAfghanistan\nEMR\nEastern Mediterranean\nAFG\nAfghanistan\n100.80\n\n\n454\nAmbient and household air pollution attributab...\nBangladesh\nSEAR\nSouth-East Asia\nBGD\nBangladesh\n100.90\n\n\n456\nAmbient and household air pollution attributab...\nMozambique\nAFR\nAfrica\nMOZ\nMozambique\n102.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3272\nAmbient and household air pollution attributab...\nPapua New Guinea\nWPR\nWestern Pacific\nPNG\nPapua New Guinea\n94.90\n\n\n3277\nAmbient and household air pollution attributab...\nSao Tome and Principe\nAFR\nAfrica\nSTP\nSao Tome and Principe\n95.96\n\n\n3278\nAmbient and household air pollution attributab...\nTogo\nAFR\nAfrica\nTGO\nTogo\n96.39\n\n\n3285\nAmbient and household air pollution attributab...\nRepublic of Moldova\nEUR\nEurope\nMDA\nRepublic of Moldova\n98.45\n\n\n3291\nAmbient and household air pollution attributab...\nHungary\nEUR\nEurope\nHUN\nHungary\n99.49\n\n\n\n\n183 rows × 7 columns\n\n\n\n\n\ngdp.head()\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nGDP per capita, PPP (constant 2017 international $)\n\n\n\n\n0\nAfghanistan\nAFG\n2002\n1280.4631\n\n\n1\nAfghanistan\nAFG\n2003\n1292.3335\n\n\n2\nAfghanistan\nAFG\n2004\n1260.0605\n\n\n3\nAfghanistan\nAFG\n2005\n1352.3207\n\n\n4\nAfghanistan\nAFG\n2006\n1366.9932\n\n\n\n\n\n\n\n\n\ngdp = gdp[(gdp['Year']==2019) & \n            (gdp['Code'].notna()) & (gdp['Code']!='World')]\n\n\ngdp\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nGDP per capita, PPP (constant 2017 international $)\n\n\n\n\n17\nAfghanistan\nAFG\n2019\n2079.9219\n\n\n49\nAlbania\nALB\n2019\n13655.6650\n\n\n81\nAlgeria\nDZA\n2019\n11627.2800\n\n\n113\nAngola\nAGO\n2019\n6602.4240\n\n\n145\nAntigua and Barbuda\nATG\n2019\n23035.6580\n\n\n...\n...\n...\n...\n...\n\n\n6215\nVanuatu\nVUT\n2019\n3070.3508\n\n\n6247\nVietnam\nVNM\n2019\n10252.0050\n\n\n6279\nWorld\nOWID_WRL\n2019\n16847.4600\n\n\n6311\nZambia\nZMB\n2019\n3372.3590\n\n\n6343\nZimbabwe\nZWE\n2019\n2203.3967\n\n\n\n\n194 rows × 4 columns\n\n\n\n\nNow we are ready to merge both.\n\ngdp_pollution = gdp.merge(death_rate, \n                        left_on='Code',\n                        right_on='SpatialDimValueCode',\n                        how='inner',validate='1:1')\n\n\ngdp_pollution\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nGDP per capita, PPP (constant 2017 international $)\nIndicator\nLocation\nParentLocationCode\nParentLocation\nSpatialDimValueCode\nLocation\nFactValueNumeric\n\n\n\n\n0\nAfghanistan\nAFG\n2019\n2079.9219\nAmbient and household air pollution attributab...\nAfghanistan\nEMR\nEastern Mediterranean\nAFG\nAfghanistan\n100.80\n\n\n1\nAlbania\nALB\n2019\n13655.6650\nAmbient and household air pollution attributab...\nAlbania\nEUR\nEurope\nALB\nAlbania\n164.80\n\n\n2\nAlgeria\nDZA\n2019\n11627.2800\nAmbient and household air pollution attributab...\nAlgeria\nAFR\nAfrica\nDZA\nAlgeria\n41.43\n\n\n3\nAngola\nAGO\n2019\n6602.4240\nAmbient and household air pollution attributab...\nAngola\nAFR\nAfrica\nAGO\nAngola\n59.47\n\n\n4\nAntigua and Barbuda\nATG\n2019\n23035.6580\nAmbient and household air pollution attributab...\nAntigua and Barbuda\nAMR\nAmericas\nATG\nAntigua and Barbuda\n21.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n171\nUzbekistan\nUZB\n2019\n7348.1470\nAmbient and household air pollution attributab...\nUzbekistan\nEUR\nEurope\nUZB\nUzbekistan\n93.10\n\n\n172\nVanuatu\nVUT\n2019\n3070.3508\nAmbient and household air pollution attributab...\nVanuatu\nWPR\nWestern Pacific\nVUT\nVanuatu\n138.20\n\n\n173\nVietnam\nVNM\n2019\n10252.0050\nAmbient and household air pollution attributab...\nViet Nam\nWPR\nWestern Pacific\nVNM\nViet Nam\n107.80\n\n\n174\nZambia\nZMB\n2019\n3372.3590\nAmbient and household air pollution attributab...\nZambia\nAFR\nAfrica\nZMB\nZambia\n63.04\n\n\n175\nZimbabwe\nZWE\n2019\n2203.3967\nAmbient and household air pollution attributab...\nZimbabwe\nAFR\nAfrica\nZWE\nZimbabwe\n80.64\n\n\n\n\n176 rows × 11 columns\n\n\n\n\nFor a quick visualization, we can use the Pandas.plot() method, that will generate a Matplotlib figure object. Our sample data is more complicated because we don’t have an X axis directly, so we will have a line graph, where the X axis is the index (i.e., row indicator) and the Y axis is the value we are plotting.\n\ngdp_pollution['FactValueNumeric'].plot()\nplt.show()\n\n\n\n\n\n\n\n\nBut this is not very informative. Let’s exploit the functionalities of the .plot() method to get a better grasp of our data.\n\ngdp_pollution.plot.scatter(x='GDP per capita, PPP (constant 2017 international $)',\n                           y='FactValueNumeric')\n\n\n\n\n\n\n\n\nThis scatterplot is starting to say something: it seems there is a negative relationship between GDP per capita and deaths due to pollution. We would have been able to produce the same graph to using Matplotlib functionalities as follows:\n\nplt.plot(gdp_pollution['GDP per capita, PPP (constant 2017 international $)'],gdp_pollution['FactValueNumeric'],'o') #'o' = scatterplot\nplt.xlabel('GDP Per Capita')  \nplt.ylabel('Air pollution attributable death rate (per 100 000 population)') \n\nText(0, 0.5, 'Air pollution attributable death rate (per 100 000 population)')\n\n\n\n\n\n\n\n\n\nWhile the Pandas functionality allows us to do direct visualizations, further customizations will be required most of the time to achieve the desired output. This is when Matplotlib comes in handy.\n\nf, axs = plt.subplots(figsize = (5,5)) # in inches. We can define a converting factor\ngdp_pollution.plot(ax=axs, \n                    x ='GDP per capita, PPP (constant 2017 international $)', \n                    y ='FactValueNumeric', \n                    kind='scatter')\n\naxs.set_xlabel('GDP Per Capita')\naxs.set_ylabel('Air pollution attributable death rate (per 100 000 population)')\naxs.set_title('Title of this subplot')\n\nx = gdp_pollution['GDP per capita, PPP (constant 2017 international $)'].values\ny = gdp_pollution['FactValueNumeric'].values\nf.suptitle('My first graph')\nf.savefig('./fig1.png')\n\n\n\n\n\n\n\n\nIt is also possible to add the labels that indicate the country code using the annotate method.\n\nf, ax = plt.subplots(figsize = (5,5)) # in inches. We can define a converting factor\ngdp_pollution.plot(ax=ax, \n                    x ='GDP per capita, PPP (constant 2017 international $)', \n                    y ='FactValueNumeric', \n                    kind='scatter')\n\nax.set_xlabel('GDP Per Capita')\nax.set_ylabel('Air pollution attributable death rate (per 100 000 population)')\nax.set_title('Title of this subplot')\n\nx = gdp_pollution['GDP per capita, PPP (constant 2017 international $)'].values\ny = gdp_pollution['FactValueNumeric'].values\nfor i in range(len(gdp_pollution)): \n    plt.annotate(gdp_pollution['Code'][i], (x[i], y[i] + 0.5), fontsize=7)\nf.suptitle('My first graph')\nf.savefig('./fig1.png')\n\n\n\n\n\n\n\n\nWe can also plot multiple subplots within the same figure.\n\nf, axs = plt.subplots(nrows=1, ncols=2, figsize = (8,5)) # Now axs contains two elements\n\ngdp_pollution.plot(ax=axs[0], \n                    x ='GDP per capita, PPP (constant 2017 international $)', \n                    y ='FactValueNumeric',\n                    kind='scatter')\n                    \naxs[0].set_xlabel('GDP Per Capita')\naxs[0].set_ylabel('Air pollution attributable death rate (per 100 000 population)')\naxs[0].set_title('Title of subplot in axs 0')\n\ngdp_pollution.plot(ax=axs[1], \n                    x ='GDP per capita, PPP (constant 2017 international $)', \n                    y ='FactValueNumeric', \n                    kind='scatter')\n\naxs[1].set_xlabel('GDP Per Capita')\naxs[1].set_ylabel('Air pollution attributable death rate (per 100 000 population)')\naxs[1].set_title('Title of subplot in axs 1')\n\nf.suptitle('My first figure with two subplots')\n\nText(0.5, 0.98, 'My first figure with two subplots')\n\n\n\n\n\n\n\n\n\nTo perform more advanced statistical graphs, we can rely on Seaborn, a library that facilitates the creation of statistical graphics by leveraging matplotlib and integrating with pandas data structures.\n\nimport seaborn as sns\nsns.set_theme(rc={'figure.figsize':(4,4)})\n\nFunctions in seaborn are classified as:\n\nFigure-level: internally create their own matplotlib figure. When we call this type of functions, they initialize its own figure, so we cannot draw them into an existing axes. To customize its axes, we need to access the Matplotlib axes that are generated within the figure and then add or modify elements.\nAxis-levels: the return plot is a matplotlib.pyplot.Axes object, which means we can use them within the Matplotlib figure set up.\n\n\nsns.relplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\",\n            y=\"FactValueNumeric\")\n\n\n\n\n\n\n\n\nIn Seaborn we can add an additional dimension in the scatterplot by using different colors for observations in different categories specifying the “hue” parameter.\n\nsns.relplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\", \n            y=\"FactValueNumeric\", \n            hue = 'ParentLocation')\n\n\n\n\n\n\n\n\nFurthermore, it is also straightforward to use different markers for each category specifying it in the “style” option. While here we are using the same variable for the differentiation, it is also possible to specify different variables in hue and style.\n\nsns.relplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\",\n            y=\"FactValueNumeric\", \n            hue = 'ParentLocation', \n            style='ParentLocation')\n\n\n\n\n\n\n\n\nTo explore more options, let’s merge our data with population at the country level.\n\npopulation = pandas.read_csv('../data/population-unwpp.csv')\n\n\npopulation = population[(population['Year']==2019) & (population['Code'].notna())]\n\n\ngdp_pollution = gdp_pollution.merge(population, \n                how='inner', on='Code', validate='1:1')\n\nNow, we can make each point have a different size depending on the population of the country\n\nsns.relplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", \n            y=\"FactValueNumeric\", \n            hue = 'ParentLocation', \n            size='Population (historical estimates)', \n            sizes = (15,250))\n\n\n\n\n\n\n\n\n\npopulation.sort_values('Population (historical estimates)')\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\nPopulation (historical estimates)\n\n\n\n\n16674\nTokelau\nTKL\n2019\n1775\n\n\n12058\nNiue\nNIU\n2019\n1943\n\n\n5562\nFalkland Islands\nFLK\n2019\n3729\n\n\n11106\nMontserrat\nMSR\n2019\n4528\n\n\n14002\nSaint Helena\nSHN\n2019\n5470\n\n\n...\n...\n...\n...\n...\n\n\n7722\nIndonesia\nIDN\n2019\n269582880\n\n\n17580\nUnited States\nUSA\n2019\n334319680\n\n\n7650\nIndia\nIND\n2019\n1383112064\n\n\n3383\nChina\nCHN\n2019\n1421864064\n\n\n18354\nWorld\nOWID_WRL\n2019\n7764951040\n\n\n\n\n237 rows × 4 columns\n\n\n\n\nSeaborn also has a functionality that allows to draw scatterplots with regression lines (regplot). The function lmplot allows also to draw the regression lines conditioning on other variables (i.e., by category)\n\nsns.regplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\",\n            y=\"FactValueNumeric\")\n\n\n\n\n\n\n\n\n\nsns.lmplot(data=gdp_pollution,\n         x=\"GDP per capita, PPP (constant 2017 international $)\",\n         y=\"FactValueNumeric\",\n         hue=\"ParentLocation\" )\n\n\n\n\n\n\n\n\nSpecifying “col” or “row” will draw separate graphs for each category.\n\nsns.lmplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\",          \n            y=\"FactValueNumeric\", \n            hue=\"ParentLocation\", \n            col =\"ParentLocation\" )\n\n\n\n\n\n\n\n\nlmplot also performs polynomial regressions.\n\nsns.lmplot(data=gdp_pollution, \n            x=\"GDP per capita, PPP (constant 2017 international $)\", \n            y=\"FactValueNumeric\", \n            order=2)\n\n\n\n\n\n\n\n\nWith a similar syntaxis, we can draw histogram and density plots, which can also be helpful to understand the distribution of continuous variables.\nFor instance, in the code below we are going to draw the histogram for GDP per capita for each parent location separately.\n\na = sns.displot(data = gdp_pollution, \n                x = \"GDP per capita, PPP (constant 2017 international $)\",\n                kind = 'hist',\n                col='ParentLocation')\n\na.set_axis_labels(\"GDP\", \"Count\")\na.set_titles(\"{col_name}\")\na.savefig('./export.png')\n\n\n\n\n\n\n\n\nSimilarly, we could have used:\n\nf, ax = plt.subplots()\nsns.histplot(data = gdp_pollution, \n            x = \"GDP per capita, PPP (constant 2017 international $)\",\n            color=\"skyblue\", \n            label=\"MaxTemp\", kde=True, ax = ax)\n\nplt.legend()\nplt.xlabel('GDP per capita')\n\nText(0.5, 0, 'GDP per capita')\n\n\n\n\n\n\n\n\n\nHeatmaps are also built-in within Seaborn and are a very popular too. to display the correlation between the variables of the dataframe. It’s like visualizing a correlation matrix with colors.\n\nsns.heatmap(gdp_pollution[[ \"GDP per capita, PPP (constant 2017 international $)\",\n                         \"FactValueNumeric\", \n                         'Population (historical estimates)']].corr())",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course foundations:\ndata management</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section2.html#practice-exercises",
    "href": "notebooks_md/section2.html#practice-exercises",
    "title": "2  Course foundations: data management",
    "section": "2.3 Practice exercises",
    "text": "2.3 Practice exercises\n\nWrite a loop that describes all columns of the credit transaction data.\n\n\nShow solution\nfor c in data.columns:\n    print(c)\n\n\nWrite a loop that plots in a different subplot the scatterplot of GDP vs mortality for each Parent Location. It should have 3 columns and 2 rows.\n\n\nShow solution\n#Defining a 2x2 grid of subplots\nfig, axs = plt.subplots(nrows=2,ncols=3,figsize=(16,10))\nplt.subplots_adjust(wspace=0.5) #adjusting white space between individual plots\n\n# get list of parent locations\nparentl = gdp_pollution['ParentLocation'].unique()\n# create dictionary of colors for each country \n# Define six colors\npalette = sns.color_palette()\n\n# Use the palette to assign colors to parent locations\ncolors = palette[:len(parentl)]\n# Create a dictionary mapping parent locations to colors\nparent_color_dict = dict(zip(parentl, colors))\n\n# convert to an array with the shapes of the figure we want so that we can loop over them \nparentl_arr= np.array(parentl)\nparentl_arr= parentl_arr.reshape(2, 3)\n\n# looping over the 2x2 grid\nfor i in range(2):\n    for j in range(3):\n\n        #extract the part of the data for that subset of countries\n        aux = gdp_pollution[gdp_pollution['ParentLocation']==parentl_arr[i,j]]\n\n        #Making the scatterplot\n        aux.plot(ax=axs[i,j], x ='GDP per capita, PPP (constant 2017 international $)', \n                y ='FactValueNumeric', kind='scatter', color=parent_color_dict[parentl_arr[i,j]])\n\n        # label axes\n\n        axs[i,j].set_xlabel('GDP per capita')\n        axs[i,j].set_ylabel('Pollution induced mortality')\n\n        # set title of subplot\n        axs[i,j].set_title(parentl_arr[i,j])\n        #Putting a dollar sign, and thousand-comma separator on x-axis labels\n        axs[i,j].xaxis.set_major_formatter('${x:,.0f}')\n\n        # increasing font size of axis labels\n        axs[i,j].tick_params(axis = 'both',labelsize=6)\n\nparentl_arr[i,j]",
    "crumbs": [
      "Introduction to Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Course foundations:\ndata management</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section3.html",
    "href": "notebooks_md/section3.html",
    "title": "3  Introducing geographic data",
    "section": "",
    "text": "3.1 To map or not to map, that is the question\nThe examples above highlight the prevalence of geographic data and geographic data science in our daily routines. Although spatial data is often equated with maps, its utility extends beyond mere cartography. Maps serve as tools to visualize spatial data, but there are numerous other analytical applications. Let’s explore some examples:",
    "crumbs": [
      "Visualization of geographic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing geographic data</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section3.html#to-map-or-not-to-map-that-is-the-question",
    "href": "notebooks_md/section3.html#to-map-or-not-to-map-that-is-the-question",
    "title": "3  Introducing geographic data",
    "section": "",
    "text": "The black belt and the election of a senator\nIn 2017, for the first time in 25 years, a Democrat, Doug Jones, was elected as senator for Alabama. The two maps below explore the possible correlation between historical factors and political outcomes. The first map indicates the distribution of counties with a significant historical presence of slavery, known as the “black belt” due to its fertile soil conducive to plantation crops and hence prone to slavery. Interestingly, the second map overlays the counties that voted for the Democrat senator.\n\n\n\nSource: Sarah Slobin, 2017\n\n\nWhile there appears to be a spatial coincidence between the two, understanding the statistical significance of this correlation is best achieved through a scatter plot analysis.\n\n\n\nSource: Policyviz, 2018\n\n\nNonetheless, while the set of maps may not be the optimal choice for visualizing correlation, they do serve as a good example to observe the phenomenon of spatial correlation. This refers to the tendency for nearby values, in this case, soil characteristics/slavery and voting behavior, to exhibit similar patterns or characteristics.\n\n\nGun ownership and suicides\nThe following maps, featured in a 2016 Washington Post article, aimed to indicate a potential correlation between gun ownership and suicide rates.\n\n\n\nSource: The Washington Post, 2016\n\n\nWhile examining both maps, it would seem that high values cluster in similar locations on both. However, understanding the correlation between the two variables is clearer when viewing the scatterplot.\n\n\n\nSource: The Urban Institute, 2016\n\n\nThe maps, however, have the advantage of representing all areas in the same size, thus avoiding visual distortions induced by differences in the sizes of States, which is a concept we will discuss later on in the course.\nThe main purpose of including these two examples is to emphasize that maps should not primarily focus on displaying correlations, as other visual plots are designed for that purpose, but rather on showing geographic patterns. Additionally, they also show that creating maps is not always straightforward, and there may be instances where distorting geography becomes necessary to effectively transmit messages.\nBut then, when should we map? Well, in some cases, there are no other options. Take, for instance, a commuting flow map, which will be one of our topics. In Spain, there are 3,214 mobility areas and approximately 66,000 origin-destination pairs. Representing this dataset poses questions. Should we use a matrix? If we have 3,214 * 3,214 potential origin-destination pairs, those are the dimensions of our matrix. What would we learn from that matrix? In cases like this, mapping is the answer! A flow map of displacements would not only summarize all this information but also potentially reveal additional insights beyond what a simple matrix representation could offer.\n\n\n\nSource: Own elaboration\n\n\nThis discussion highlights the importance of knowing when to use maps effectively. Historically, there have been excellent examples of maps that originated breakthroughs. John Snow’s cholera map from 1854 is a classic case.\nIn 1854, a cholera outbreak in Soho, London, killed over 120 people in just three days. Dr. John Snow was skeptical of the prevailing belief that cholera spread through the air. To better understand the outbreak and its diffusion, he identified the homes affected by the disease and plotted the locations of the dead on a map. He discovered that these locations clustered around a pump on Broad Street. This analysis helped him identify the source of the epidemic: a contaminated water well. He suggested shutting down the well, which led to saving many lives.\n\n\n\nSource: John Snow cholera Map 1854, digitized by R. Wilson\n\n\nThis story is famous because it is often considered the first epidemiological analysis of disease, demonstrating how environmental factors can contribute to its spread and the first geographic analysis of disease by plotting points on a map and looking for relationships. In fact, this discovery revolutionized public health by challenging the prevailing belief that diseases spread through the air.\nMapping can also be used to provide insights into social and economic conditions. Charles Booth (1840-1916) was dissatisfied with the Census information on poverty in London, believing it to be inaccurate. He conducted his own survey investigating Londoners’ workplaces, working conditions, homes, and urban environments.\n\n\n\nSource: Maps Descriptive of London Poverty, Charles Booth (1886-1903)\n\n\nThis research culminated in the Inquiry into the Life and Labour of the People in London (1886-1903). One of the most well-known pieces of this work is the Maps Descriptive of London Poverty, which maps poverty and wealth levels street by street. These maps are an early example of social cartography, as each street is colored according to the income and social class of its inhabitants, ranging from “Lowest class. Vicious, semi-criminal.” to “Upper-middle and upper classes. Wealthy”.\nVisualizing geographic data involves more than just making maps. Take aerial images of two regions with distinctly different built environments, for instance. These differences would be visible from the images. But how could we summarize them? By rearranging the pixels based on color, we could use color gradients to fully characterize these areas. What if we could use these gradients to gain more insights? Since the gradients reflect the built environment, they also encapsulate characteristics of the area’s inhabitants. This connection could be extended to analyze political behaviors, providing a way to link the built environment with political patterns. This innovation was precisely behind the idea of this New York Times article, in which they mapped vote shares to color gradients to show political voting behavior along the urban and rural divide.\n\n\n\nSource: The New York Times",
    "crumbs": [
      "Visualization of geographic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing geographic data</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section3.html#spatial-data-is-special",
    "href": "notebooks_md/section3.html#spatial-data-is-special",
    "title": "3  Introducing geographic data",
    "section": "3.2 Spatial data is special",
    "text": "3.2 Spatial data is special\nLocation is treated in a unique way across various dimensions:\n\nComputationally, managing and analyzing spatial data requires using specific software (GIS). GIS can be defined in different ways (see the definitions at GISGeography) but all definitions share the following elements:\n\nComputer Systems: Computers have changed the way we work with geographic data.\nGeographic Data: GIS are designed to work with location-based data.\nInformation: They integrate geographic data and their descriptive attributes. They allow visualizing them in the form of maps.\nAnalytical: They allow analyzing and drawing conclusions from geographic data and underlying processes, beyond just mapping.\n\nSo, we could say that a GIS is a software to connect data with geography through techniques of visualization and spatial analysis.\nVisually, spatial data allows for the creation of maps. But as we just discussed, we may not always want to create maps. Keep in mind that maps, just like models, are representations of complex phenomena, relying on certain assumptions. Thus, being representations of reality all of them will be somehow wrong, but some of them will also be useful (Ord, 2010).\nStatistically, location requires consideration of spatial dependence and spatial errors. According to the Laws of Geography articulated by Tobler (1970, 1999), “Everything is related to everything else, but near things are more related than distant things,” and “Phenomena external to a geographic area affect what happens within it.” All of this imply that in geographic setups, most of the standard statistical assumptions we rely on, will not hold.",
    "crumbs": [
      "Visualization of geographic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing geographic data</span>"
    ]
  },
  {
    "objectID": "notebooks_md/section3.html#spatial-data-sources",
    "href": "notebooks_md/section3.html#spatial-data-sources",
    "title": "3  Introducing geographic data",
    "section": "3.3 Spatial data sources:",
    "text": "3.3 Spatial data sources:",
    "crumbs": [
      "Visualization of geographic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducing geographic data</span>"
    ]
  }
]